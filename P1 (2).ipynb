{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yfpA4Z5ADdok",
        "jQKJ4zc9UyOc"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CMT309 - Computational Data Science - Data Science Portfolio**"
      ],
      "metadata": {
        "id": "t_hYgOgUzIwe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCAPWR2s3nzA"
      },
      "source": [
        "# Part 1 - Text Data Analysis (45 marks)\n",
        "\n",
        "In this question you will write Python code for processing, analyzing and understanding the social network **Reddit** (www.reddit.com). Reddit is a platform that allows users to upload posts and comment on them, and is divided in _subreddits_, often covering specific themes or areas of interest (for example, [world news](https://www.reddit.com/r/worldnews/), [ukpolitics](https://www.reddit.com/r/ukpolitics/) or [nintendo](https://www.reddit.com/r/nintendo)). You are provided with a subset of Reddit with posts from Covid-related subreddits (e.g., _CoronavirusUK_ or _NoNewNormal_), as well as randomly selected subreddits (e.g., _donaldtrump_ or _razer_).\n",
        "\n",
        "The `csv` dataset you are provided contains one row per post, and has information about three entities: **posts**, **users** and **subreddits**. The column names are self-explanatory: columns starting with the prefix `user_` describe users, those starting with the prefix `subr_` describe subreddits, the `subreddit` column is the subreddit name, and the rest of the columns are post attributes (`author`, `posted_at`, `title` and post text - the `selftext` column-, number of comments - `num_comments`, `score`, etc.).\n",
        "\n",
        "In this exercise, you are asked to perform a number of operations to gain insights from the data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## P1.0) Suggested/Required Imports"
      ],
      "metadata": {
        "id": "jlssd-CQOOXE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pm74v1u4d6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cb4ea7c-87c0-41f5-a1a0-e41aeca5ac64"
      },
      "source": [
        "# suggested imports\n",
        "import pandas as pd\n",
        "from nltk.tag import pos_tag\n",
        "import re\n",
        "from collections import defaultdict,Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "tqdm.pandas()\n",
        "from ast import literal_eval\n",
        "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfNsDQ253nzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a80a0504-990e-44a6-8a9d-c1f1c512175f"
      },
      "source": [
        "from urllib import request\n",
        "import pandas as pd\n",
        "module_url = f\"https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_22.csv\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  a = f.read()\n",
        "  outf.write(a.decode('utf-8'))\n",
        "df = pd.read_csv('data_portfolio_22.csv')\n",
        "# this fills empty cells with empty strings\n",
        "df = df.fillna('')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_22.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CNfbxg2X3nzK",
        "outputId": "3932d3a9-4392-434d-8796-041b70bd579d"
      },
      "source": [
        "df"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           author            posted_at  num_comments  score  \\\n",
              "0      -Howitzer-  2020-08-17 20:26:04            19      1   \n",
              "1      -Howitzer-  2020-07-06 17:01:48             1      3   \n",
              "2      -Howitzer-  2020-09-09 02:29:02             3      1   \n",
              "3      -Howitzer-  2020-06-23 23:02:39             2      1   \n",
              "4      -Howitzer-  2020-08-07 04:13:53            32    622   \n",
              "...           ...                  ...           ...    ...   \n",
              "19935     zqrwiel  2020-07-23 16:39:15            11    246   \n",
              "19936     zqrwiel  2020-12-15 11:25:07            39      1   \n",
              "19937     zqrwiel  2020-12-27 13:57:49            15      1   \n",
              "19938     zqrwiel  2020-12-29 12:07:10             6      1   \n",
              "19939     zqrwiel  2021-01-21 16:47:13            23      1   \n",
              "\n",
              "                                                selftext subr_created_at  \\\n",
              "0                                                             2009-04-29   \n",
              "1                                                             2009-04-29   \n",
              "2                                                             2009-04-29   \n",
              "3                                                             2009-04-29   \n",
              "4                                                             2009-04-29   \n",
              "...                                                  ...             ...   \n",
              "19935                                                         2009-04-13   \n",
              "19936  Then I think we might get 18 songs, outro usua...      2009-04-13   \n",
              "19937  He has 25songs to perform plus the additional ...      2009-04-13   \n",
              "19938     I got goose[***]ps just by thinking about it ðŸ˜¬      2009-04-13   \n",
              "19939                                                         2009-04-13   \n",
              "\n",
              "                                        subr_description  \\\n",
              "0                           Subreddit about Donald Trump   \n",
              "1                           Subreddit about Donald Trump   \n",
              "2                           Subreddit about Donald Trump   \n",
              "3                           Subreddit about Donald Trump   \n",
              "4                           Subreddit about Donald Trump   \n",
              "...                                                  ...   \n",
              "19935  A subreddit dedicated to the discussion of hip...   \n",
              "19936  A subreddit dedicated to the discussion of hip...   \n",
              "19937  A subreddit dedicated to the discussion of hip...   \n",
              "19938  A subreddit dedicated to the discussion of hip...   \n",
              "19939  A subreddit dedicated to the discussion of hip...   \n",
              "\n",
              "                                           subr_faved_by  subr_numb_members  \\\n",
              "0      ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "1      ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "2      ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "3      ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "4      ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "...                                                  ...                ...   \n",
              "19935  ['solex125', 'redreddington22', 'HibikiSS', 'k...               8740   \n",
              "19936  ['solex125', 'redreddington22', 'HibikiSS', 'k...               8740   \n",
              "19937  ['solex125', 'redreddington22', 'HibikiSS', 'k...               8740   \n",
              "19938  ['solex125', 'redreddington22', 'HibikiSS', 'k...               8740   \n",
              "19939  ['solex125', 'redreddington22', 'HibikiSS', 'k...               8740   \n",
              "\n",
              "       subr_numb_posts     subreddit  \\\n",
              "0               796986   donaldtrump   \n",
              "1               796986   donaldtrump   \n",
              "2               796986   donaldtrump   \n",
              "3               796986   donaldtrump   \n",
              "4               796986   donaldtrump   \n",
              "...                ...           ...   \n",
              "19935           630857  playboicarti   \n",
              "19936           630857  playboicarti   \n",
              "19937           630857  playboicarti   \n",
              "19938           630857  playboicarti   \n",
              "19939           630857  playboicarti   \n",
              "\n",
              "                                                   title  \\\n",
              "0      BREAKING: Trump to begin hiding in mailboxes t...   \n",
              "1                                    Joe Biden's America   \n",
              "2      4 more years and we can erase his legacy for g...   \n",
              "3      Revelation 9:6 [Transhumanism: The New Religio...   \n",
              "4                                         LOOK HERE, FAT   \n",
              "...                                                  ...   \n",
              "19935                                          carti why   \n",
              "19936                           If uzi on track 3 and 16   \n",
              "19937          Man cartiâ€™s concerts are gonna be long af   \n",
              "19938  Canâ€™t wait to see Carti going full rage mode o...   \n",
              "19939           [OFFTOPIC] have you seen that new LV? ðŸ˜‚ðŸ’€   \n",
              "\n",
              "       total_awards_received  upvote_ratio  user_num_posts user_registered_at  \\\n",
              "0                          0          1.00            4661         2012-11-09   \n",
              "1                          0          0.67            4661         2012-11-09   \n",
              "2                          0          1.00            4661         2012-11-09   \n",
              "3                          0          1.00            4661         2012-11-09   \n",
              "4                          0          0.88            4661         2012-11-09   \n",
              "...                      ...           ...             ...                ...   \n",
              "19935                      0          1.00            1883         2014-02-12   \n",
              "19936                      0          1.00            1883         2014-02-12   \n",
              "19937                      0          1.00            1883         2014-02-12   \n",
              "19938                      0          1.00            1883         2014-02-12   \n",
              "19939                      0          1.00            1883         2014-02-12   \n",
              "\n",
              "       user_upvote_ratio  \n",
              "0              -0.658599  \n",
              "1              -0.658599  \n",
              "2              -0.658599  \n",
              "3              -0.658599  \n",
              "4              -0.658599  \n",
              "...                  ...  \n",
              "19935           0.861626  \n",
              "19936           0.861626  \n",
              "19937           0.861626  \n",
              "19938           0.861626  \n",
              "19939           0.861626  \n",
              "\n",
              "[19940 rows x 17 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-717187a6-0f98-49c5-a4ca-90b31251f902\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>posted_at</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>score</th>\n",
              "      <th>selftext</th>\n",
              "      <th>subr_created_at</th>\n",
              "      <th>subr_description</th>\n",
              "      <th>subr_faved_by</th>\n",
              "      <th>subr_numb_members</th>\n",
              "      <th>subr_numb_posts</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>title</th>\n",
              "      <th>total_awards_received</th>\n",
              "      <th>upvote_ratio</th>\n",
              "      <th>user_num_posts</th>\n",
              "      <th>user_registered_at</th>\n",
              "      <th>user_upvote_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-08-17 20:26:04</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>BREAKING: Trump to begin hiding in mailboxes t...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-07-06 17:01:48</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>Joe Biden's America</td>\n",
              "      <td>0</td>\n",
              "      <td>0.67</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-09-09 02:29:02</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>4 more years and we can erase his legacy for g...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-06-23 23:02:39</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>Revelation 9:6 [Transhumanism: The New Religio...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-08-07 04:13:53</td>\n",
              "      <td>32</td>\n",
              "      <td>622</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>LOOK HERE, FAT</td>\n",
              "      <td>0</td>\n",
              "      <td>0.88</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19935</th>\n",
              "      <td>zqrwiel</td>\n",
              "      <td>2020-07-23 16:39:15</td>\n",
              "      <td>11</td>\n",
              "      <td>246</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-13</td>\n",
              "      <td>A subreddit dedicated to the discussion of hip...</td>\n",
              "      <td>['solex125', 'redreddington22', 'HibikiSS', 'k...</td>\n",
              "      <td>8740</td>\n",
              "      <td>630857</td>\n",
              "      <td>playboicarti</td>\n",
              "      <td>carti why</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1883</td>\n",
              "      <td>2014-02-12</td>\n",
              "      <td>0.861626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19936</th>\n",
              "      <td>zqrwiel</td>\n",
              "      <td>2020-12-15 11:25:07</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>Then I think we might get 18 songs, outro usua...</td>\n",
              "      <td>2009-04-13</td>\n",
              "      <td>A subreddit dedicated to the discussion of hip...</td>\n",
              "      <td>['solex125', 'redreddington22', 'HibikiSS', 'k...</td>\n",
              "      <td>8740</td>\n",
              "      <td>630857</td>\n",
              "      <td>playboicarti</td>\n",
              "      <td>If uzi on track 3 and 16</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1883</td>\n",
              "      <td>2014-02-12</td>\n",
              "      <td>0.861626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19937</th>\n",
              "      <td>zqrwiel</td>\n",
              "      <td>2020-12-27 13:57:49</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>He has 25songs to perform plus the additional ...</td>\n",
              "      <td>2009-04-13</td>\n",
              "      <td>A subreddit dedicated to the discussion of hip...</td>\n",
              "      <td>['solex125', 'redreddington22', 'HibikiSS', 'k...</td>\n",
              "      <td>8740</td>\n",
              "      <td>630857</td>\n",
              "      <td>playboicarti</td>\n",
              "      <td>Man cartiâ€™s concerts are gonna be long af</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1883</td>\n",
              "      <td>2014-02-12</td>\n",
              "      <td>0.861626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19938</th>\n",
              "      <td>zqrwiel</td>\n",
              "      <td>2020-12-29 12:07:10</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>I got goose[***]ps just by thinking about it ðŸ˜¬</td>\n",
              "      <td>2009-04-13</td>\n",
              "      <td>A subreddit dedicated to the discussion of hip...</td>\n",
              "      <td>['solex125', 'redreddington22', 'HibikiSS', 'k...</td>\n",
              "      <td>8740</td>\n",
              "      <td>630857</td>\n",
              "      <td>playboicarti</td>\n",
              "      <td>Canâ€™t wait to see Carti going full rage mode o...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1883</td>\n",
              "      <td>2014-02-12</td>\n",
              "      <td>0.861626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19939</th>\n",
              "      <td>zqrwiel</td>\n",
              "      <td>2021-01-21 16:47:13</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-13</td>\n",
              "      <td>A subreddit dedicated to the discussion of hip...</td>\n",
              "      <td>['solex125', 'redreddington22', 'HibikiSS', 'k...</td>\n",
              "      <td>8740</td>\n",
              "      <td>630857</td>\n",
              "      <td>playboicarti</td>\n",
              "      <td>[OFFTOPIC] have you seen that new LV? ðŸ˜‚ðŸ’€</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1883</td>\n",
              "      <td>2014-02-12</td>\n",
              "      <td>0.861626</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19940 rows Ã— 17 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-717187a6-0f98-49c5-a4ca-90b31251f902')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-717187a6-0f98-49c5-a4ca-90b31251f902 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-717187a6-0f98-49c5-a4ca-90b31251f902');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyQyR27z48nr"
      },
      "source": [
        "## P1.1 - Text data processing (20 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfpA4Z5ADdok"
      },
      "source": [
        "### P1.1.1 - Offensive authors per subreddit (5 marks)\n",
        "\n",
        "As you will see, the dataset contains a lot of strings of the form `[***]`. These have been used to mask (or remove) swearwords to make it less offensive. We are interested in finding those users that have posted at least one swearword in each subreddit. We do this by counting occurrences of the `[***]` string in the `selftext` column (we can assume that an occurrence of `[***]` equals a swearword in the original dataset).\n",
        "\n",
        "**What to implement:** A function `offensive_authors(df)` that takes as input the original dataframe and returns a dataframe of the form below, where each row contains authors that posted at least one swearword in the corresponding subreddit.\n",
        "\n",
        "```\n",
        "subreddit\tauthor\n",
        "0\t40kLore\tCross_Ange\n",
        "1\t40kLore\tDaRandomGitty2\n",
        "2\t40kLore\tEMB1981\n",
        "3\t40kLore\tEvoxrus_XV\n",
        "4\t40kLore\tGrtrshop\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def offensive_authors(df):\n",
        "  # your answer here\n",
        "  return df[df['selftext'].str.contains(\"[***]\")][['subreddit','author']]\n",
        "  "
      ],
      "metadata": {
        "id": "9oby6Xbz59F0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offensive_authors(df)"
      ],
      "metadata": {
        "id": "FdiKSgWa9dKO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "7c23dce5-3e0f-4fa0-a9d3-d6254824963e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          subreddit              author\n",
              "44       conspiracy            0naptoon\n",
              "47       conspiracy            0naptoon\n",
              "51       conspiracy  10100011a10100011a\n",
              "76       conspiracy         13followsMe\n",
              "91       conspiracy     2012ronpaul2012\n",
              "...             ...                 ...\n",
              "19841  playboicarti          yoda_[***]\n",
              "19849  playboicarti          yoda_[***]\n",
              "19930  playboicarti             zqrwiel\n",
              "19933  playboicarti             zqrwiel\n",
              "19938  playboicarti             zqrwiel\n",
              "\n",
              "[1200 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bb4bc232-428c-4b27-9e1b-9e21133db5e1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subreddit</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>conspiracy</td>\n",
              "      <td>0naptoon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>conspiracy</td>\n",
              "      <td>0naptoon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>conspiracy</td>\n",
              "      <td>10100011a10100011a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>conspiracy</td>\n",
              "      <td>13followsMe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>conspiracy</td>\n",
              "      <td>2012ronpaul2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19841</th>\n",
              "      <td>playboicarti</td>\n",
              "      <td>yoda_[***]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19849</th>\n",
              "      <td>playboicarti</td>\n",
              "      <td>yoda_[***]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19930</th>\n",
              "      <td>playboicarti</td>\n",
              "      <td>zqrwiel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19933</th>\n",
              "      <td>playboicarti</td>\n",
              "      <td>zqrwiel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19938</th>\n",
              "      <td>playboicarti</td>\n",
              "      <td>zqrwiel</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1200 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb4bc232-428c-4b27-9e1b-9e21133db5e1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bb4bc232-428c-4b27-9e1b-9e21133db5e1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bb4bc232-428c-4b27-9e1b-9e21133db5e1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhZ3u5aS3rrm"
      },
      "source": [
        "### P1.1.2 - Most common trigrams per subreddit (15 marks)\n",
        "\n",
        "We are interested in learning about _the ten most frequent trigrams_ (a [trigram](https://en.wikipedia.org/wiki/Trigram) is a sequence of three consecutive words) in each subreddit's content. You must compute these trigrams on both the `selftext` and `title` columns. Your task is to generate a Python dictionary of the form:\n",
        "\n",
        "```\n",
        "{subreddit1: [(trigram1, freq1), (trigram2, freq2), ... , (trigram3, freq10)],\n",
        "subreddit1: [(trigram1, freq1), (trigram2, freq2), ... , (trigram3, freq10)],\n",
        "...\n",
        "subreddit63: [(trigram1, freq1), (trigram2, freq2), ... , (trigram3, freq10)],}\n",
        "```\n",
        "\n",
        "That is, for each subreddit, the 10 most frequent trigrams and their frequency, stored in a list of tuples. Each trigram will be stored also as a tuple containing 3 strings.\n",
        "\n",
        "**What to implement**: A function `get_tris(df, stopwords_list, punctuation_list)` that will take as input the original dataframe, a list of stopwords and a list of punctuation signs (e.g., `?` or `!`), and will return a python dictionary with the above format. Your function must implement the following steps in order:\n",
        "\n",
        "- (**1 mark**) Create a new dataframe called `newdf` with only `subreddit`, `title` and `selftext` columns.\n",
        "- (**1 mark**) Add a new column to `newdf` called `full_text`, which will contain `title` and `selftext` concatenated with the string `.` (a full stop) followed by a space. That, is `A simple title` and `This is a text body` would be `A simple title. This is a text body`.\n",
        "- (**1 mark**) Remove all occurrences of the following strings from `full_text`. You must do this without creating a new column:\n",
        "  - `[***]`\n",
        "  - `&amp;`\n",
        "  - `&gt;`\n",
        "  - `https`\n",
        "- (**1 mark**) You must also remove all occurrences of at least three consecutive hyphens, for example, you should remove strings like `---`, `----`, `-----`, etc., but not `--` and not `-`.\n",
        "- (**1 mark**) Tokenize the contents of the `full_text` column after lower casing (removing all capitalization). You should use the `word_tokenize` function in `nltk`. Add the results to a new column called `full_text_tokenized`.\n",
        "- (**2 mark**) Remove all tokens that are either stopwords or punctuation from `full_text_tokenized` and store the results in a new column called `full_text_tokenized_clean`. _See Note 1_.\n",
        "- (**2 marks**) Create a new dataframe called `adf` (which will stand for _aggregated dataframe_), which will have one row per subreddit (i.e., 63 rows), and will have two columns: `subreddit` (the subreddit name), and `all_words`, which will be a big list with all the words that belong to that subreddit as extracted from the `full_text_tokenized_clean`.\n",
        "- (**3 marks**) Obtain trigram counts, which will be stored in a dictionary where each `key` will be a trigram (a `tuple` containing 3 consecutive tokens), and each `value` will be their overall frequency in that subreddit. You are  encouraged to use functions from the `nltk` package, although you can choose any approach to solve this part.\n",
        "- (**3 marks**) Finally, use the information you have in `adf` for generating the desired dictionary, and return it. _See Note 2_.\n",
        "\n",
        "Note 1. You can obtain stopwords and punctuation as follows.\n",
        "- Stopwords: \n",
        "```\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('english')\n",
        "```\n",
        "- Punctuation:\n",
        "```\n",
        "import string\n",
        "punctuation = list(string.punctuation)\n",
        "```\n",
        "\n",
        "Note 2. You do not have to apply an additional ordering when there are several trigrams with the same frequency."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports here for extra clarity\n",
        "from nltk.corpus import stopwords as sw\n",
        "import string\n",
        "import warnings\n",
        "def get_tris(df, stopwords_list, punctuation_list):\n",
        "  # 1 MARK - create new df with only relevant columns\n",
        "  newdf = df[['subreddit','title', 'selftext']]\n",
        "  \n",
        "  # 1 MARK - concatenate title and selftext\n",
        "  # 1 MARK for string replacement\n",
        "  newdf['full_text'] = df.apply(lambda row: f\"{row.title}. {row.selftext}\", axis=1)\n",
        "  \n",
        "  # 1 MARK for regex replacement - remove the strings \"[***]\", \"&amp;\", \"&gt;\" and \"https\", also at least three consecutive dashes\n",
        "  prohibitedWords = ['[***]', '&amp;', '&gt;', 'https']\n",
        "  regex1 = re.compile('|'.join(map(re.escape, prohibitedWords)))\n",
        "  newdf['full_text'] = newdf.apply(lambda row: regex1.sub(\"\", row.full_text), axis=1)\n",
        "  newdf['full_text'] = newdf.apply(lambda row: re.sub(\"/-{3,}/\" ,\"\", row.full_text), axis=1)\n",
        "\n",
        "  # 1 MARK - lower case, tokenize, and add result to full_text_tokenize\n",
        "  newdf['full_text_tokenized'] = newdf.apply(lambda row: word_tokenize(row.full_text.lower()), axis=1)\n",
        "  \n",
        "  # 2 MARKS - clean the full_text_tokenized column by iterating over each word and discarding if it's either a stopword or punctuation\n",
        "  newdf['full_text_tokenized_clean'] = newdf.apply(lambda row: [x for x in row.full_text_tokenized if x not in stopwords_list + punctuation_list], axis=1)\n",
        "  \n",
        "  # 2 MARKS - create new aggregated dataframe by concatenating all full_text_tokenized_clean values - rename columns as requested\n",
        "  adf = newdf.groupby('subreddit').agg({'full_text_tokenized_clean': 'sum'})\n",
        "  \n",
        "  # 3 MARKS - create new Series object by piping nltk's FreqDist and trigrams functions into all_words\n",
        "  adf['all_words'] = adf.apply(lambda row: nltk.FreqDist(nltk.trigrams(row.full_text_tokenized_clean)).most_common(10), axis=1)\n",
        "  \n",
        "  # 3 MARKS - create output dictionary by zipping subreddit column from adf and tri_counts into a list of tuples, then passing dict()\n",
        "  # the top 10 most frequent ngrams are obtained by calling sorted() on tri_counts and keeping only the top 10 elements\n",
        "  out_dict = {}\n",
        "  for i, row in adf.iterrows():\n",
        "    out_dict[i] = row['all_words']\n",
        "\n",
        "  return out_dict\n",
        "  "
      ],
      "metadata": {
        "id": "A0NeN7uGftfY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get stopwords as list\n",
        "sw = sw.words('english')\n",
        "# get punctuation as list\n",
        "p = list(string.punctuation)\n",
        "# optional lines for adding the below line to avoid the SettingWithCopyWarning\n",
        "warnings.filterwarnings('ignore')\n",
        "get_tris(df, sw, p)"
      ],
      "metadata": {
        "id": "jLHfnUK_g5vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8E010UbQyML"
      },
      "source": [
        "## P1.2 - Answering questions with pandas (15 marks)\n",
        "\n",
        "In this question, your task is to use pandas to answer questions about the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZmG2VIYQ93I"
      },
      "source": [
        "### P1.2.1 - Authors that post highly commented posts (3 marks)\n",
        "\n",
        "Find the top 1000 most commented posts. Then, obtain the names of the authors that have at least 3 posts among these posts.\n",
        "\n",
        "**What to implement:** Implement a function `find_popular_authors(df)` that takes as input the original dataframe and returns a list strings, where each string is the name of authors that satisfy the above criteria."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_popular_authors(df):\n",
        "  # your answer here\n",
        "  sorted_df = df.sort_values(by=['num_comments'], ascending=False).head(1000)\n",
        "  out_list = []\n",
        "  for i, row in sorted_df.iterrows():\n",
        "    if row['subr_numb_posts'] >= 3:\n",
        "      out_list.append(row['author'])  \n",
        "  return list(set(out_list))"
      ],
      "metadata": {
        "id": "URKIW6oMvrYu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_popular_authors(df)"
      ],
      "metadata": {
        "id": "tmsJyZ1_xpGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51e11f78-85a1-4faf-c6b9-986fbdea35a1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['skuzgang',\n",
              " 'SemperPereunt',\n",
              " 'DoseofTrainwreckstv',\n",
              " 'morememesplease',\n",
              " 'joshlreddit',\n",
              " 'illuminata8',\n",
              " 'chakalakasp',\n",
              " 'hilltopye',\n",
              " 'PedaleFrancisca',\n",
              " 'nnnarbz',\n",
              " 'mkbt',\n",
              " 'Robin_7883',\n",
              " 'throwaway888253',\n",
              " 'epiphanyx99',\n",
              " '2020c[***]er[***]',\n",
              " 'dsbwayne',\n",
              " 'Fr1sk3r',\n",
              " 'JLBesq1981',\n",
              " 'covidditing',\n",
              " 'DeadEndFred',\n",
              " 'AbominableAnon',\n",
              " 'maize-n-blue98',\n",
              " 'theduck1893',\n",
              " 'Rude-Mushroom',\n",
              " 'BebeFanMasterJ',\n",
              " 't0pt0p',\n",
              " 'agoodsolidthrowaway',\n",
              " 'ctrlaltdelmarva',\n",
              " 'daysgoneby27',\n",
              " 'exmoor456',\n",
              " 'Underachlever',\n",
              " 'retalaznstyle',\n",
              " 'NewAltWhoThis',\n",
              " 'SupCJ',\n",
              " 'edgar-reed',\n",
              " 'mepper',\n",
              " 'Coffeboii4real',\n",
              " 'Fuzier',\n",
              " 'itsreallyreallytrue',\n",
              " 'q[***]erras',\n",
              " 'icedpickles',\n",
              " 'Cicada200',\n",
              " 'BlindingTwilight',\n",
              " 'No-NPC-HERE',\n",
              " 'DogMeatTalk',\n",
              " 'kent_k',\n",
              " 'SlobBarker',\n",
              " 'spock23',\n",
              " 'bmac3434',\n",
              " 'Gdileavemealone',\n",
              " 'iSlingShlong',\n",
              " 'AintEverLucky',\n",
              " 'Avyxyva',\n",
              " 'Venus230',\n",
              " 'nycsellit4me',\n",
              " 'gabrysanto',\n",
              " 'acmed',\n",
              " 'Sabremesh',\n",
              " 'doththedoth',\n",
              " 'cfbovernfl',\n",
              " 'TheJeck',\n",
              " 'speckz',\n",
              " '_NoSoup4You',\n",
              " 'Pretty_iin_Pink',\n",
              " 'IanMazgelis',\n",
              " 'HighRoller390',\n",
              " 'vanish619',\n",
              " 'Salramm01',\n",
              " 'ReactionAndy',\n",
              " 'T1D2',\n",
              " 'Nihilist911',\n",
              " 'None',\n",
              " 'Facerealityalready',\n",
              " 'Singularitytracker',\n",
              " 'ArminTv',\n",
              " 'lexytheblasian',\n",
              " 'johnruby',\n",
              " 'Madd-Nigrulo',\n",
              " 'FromMN2AZ2017',\n",
              " 'foodforthinks',\n",
              " 'Przemek0980',\n",
              " 'mliash',\n",
              " 'IronWolve',\n",
              " 'lanqian',\n",
              " 'thriftyqueen',\n",
              " 'Eli-Azrael',\n",
              " 'AddictedReddit',\n",
              " 'Leg_holes',\n",
              " 'MacPepper',\n",
              " 'lgigi69',\n",
              " 'justbeho',\n",
              " 'Turbostrider27',\n",
              " 'burtzev',\n",
              " 'bgny',\n",
              " 'HippolasCage',\n",
              " 'InternetCaesar',\n",
              " 'Vegardand',\n",
              " 'ArtistDecor',\n",
              " 'DWillGlobal',\n",
              " 'Gringo_Please',\n",
              " 'Skullzrulerz',\n",
              " 'lightiggy',\n",
              " '_gschaftlhuaba',\n",
              " 'DominusDK',\n",
              " 'faab64',\n",
              " 'Gboard2',\n",
              " 'Balls_of_Adamanthium',\n",
              " 'elt0p0',\n",
              " 'BadlyShapedKnees',\n",
              " 'MysteriiousComposer',\n",
              " 'PurestVideos',\n",
              " 'Looddak',\n",
              " 'mobo392',\n",
              " 'SUPERGUESSOUS',\n",
              " 'dadboddadjokes',\n",
              " 'FredoSosa',\n",
              " 'BumblesAZ',\n",
              " '-ZeuS--',\n",
              " 'TheFearlessWarrior',\n",
              " 'Wagamaga',\n",
              " 'ufgman',\n",
              " 'le_br1t',\n",
              " 'stuuked',\n",
              " 'IsaacM42',\n",
              " 'Infjuk',\n",
              " 'Chap82',\n",
              " 'CaptainTomato21',\n",
              " 'shaman1011',\n",
              " 'diveonfire',\n",
              " 'ImNotHereStopAsking',\n",
              " 'atomolayanatomay',\n",
              " 'jigsawmap',\n",
              " 'globalhumanism',\n",
              " 'jerish29',\n",
              " 'dukey',\n",
              " 'Chobits_',\n",
              " 'eff50',\n",
              " 's4nskrit',\n",
              " 'cdillon42',\n",
              " 'BaldLurker',\n",
              " 'society0',\n",
              " 'Mcnst',\n",
              " 'scydude',\n",
              " 'KenshiroTheKid',\n",
              " 'sour_creme',\n",
              " 'memezzer',\n",
              " 'bionista',\n",
              " 'theinfinitelight',\n",
              " 'ErinInTheMorning',\n",
              " 'cottoncandy240',\n",
              " 'PlenitudeOpulence',\n",
              " 'Zenbach',\n",
              " 'LisaMck041',\n",
              " 'GrandpaChainz',\n",
              " 'Saint_eX',\n",
              " 'okabe_rintau',\n",
              " 'Grtrshop',\n",
              " 'Glad-Software',\n",
              " 'Frocharocha',\n",
              " 'Riomegon',\n",
              " 'Mister_That_Guy',\n",
              " 'iDannyEL',\n",
              " 'Juicyjackson',\n",
              " 'ghostmeharder',\n",
              " 'buoninachos',\n",
              " 'OgranismAtWork',\n",
              " 'ReginaldJohnston',\n",
              " 'FearLess_Alpha',\n",
              " 'TLDR_Swinton',\n",
              " 'ilove[***]77',\n",
              " 'samzz41',\n",
              " 'manmeet10',\n",
              " 'jollygreenscott91',\n",
              " 'SamCFC___',\n",
              " 'Jah_Wobble',\n",
              " 'SSBM_Cob',\n",
              " 'TrumpSharted',\n",
              " 'amichail',\n",
              " 'arctic-gold-digger',\n",
              " 'royroy',\n",
              " 'mostaksaif',\n",
              " 'weed[***]',\n",
              " 'Kiddy7180',\n",
              " 'strngerdngermaus',\n",
              " 'try-bi-sum-[***]',\n",
              " 'furretguy',\n",
              " 'XDitto',\n",
              " 'BlueIce5',\n",
              " 'AllisonGator',\n",
              " 'NewShooterComingOut',\n",
              " 'helpfuldare',\n",
              " 'magnolyuh',\n",
              " 'progress18',\n",
              " 'toneii',\n",
              " 'ChiefAzrael',\n",
              " 'Riptide559',\n",
              " 'mynameisalex1',\n",
              " 'Dannerz',\n",
              " 'rickmartingt',\n",
              " 'tefunka',\n",
              " 'tizthewizard',\n",
              " 'return2ozma',\n",
              " 'very_zesty_zach',\n",
              " 'pantagathus01',\n",
              " 'SverreTM',\n",
              " 'into_the_[***]e',\n",
              " 'FunPeach0',\n",
              " 'Suxdrip',\n",
              " 'GlobalConcentrate7',\n",
              " 'jblackmiser',\n",
              " 'TampaBayTimes',\n",
              " 'Molire',\n",
              " 'jakerepp15',\n",
              " 'Canuknucklehead',\n",
              " 'MisterSpiny',\n",
              " 'missuncleben',\n",
              " 'n0b0dyn00ne001',\n",
              " 'factfind',\n",
              " 'CLO_Junkie',\n",
              " 'Roka-chan',\n",
              " 'shylock92008',\n",
              " 'lickmy[***]609',\n",
              " 'platxerath',\n",
              " 'AristonD',\n",
              " 'FringeCenterPodcast',\n",
              " 'quietpilgrim',\n",
              " 'TSCSN',\n",
              " 'HeinieKa[***]ler',\n",
              " 'rspix000',\n",
              " 'skenoj',\n",
              " 'abdouh15',\n",
              " 'KimJongMakeEmSayUn',\n",
              " 'opiumcx',\n",
              " 'esberat',\n",
              " 'mission_improbables',\n",
              " 'HugeDetective0',\n",
              " 'eastbayted',\n",
              " 'koolman631',\n",
              " 'SayLittleDoMuch',\n",
              " 'atomic[***]e',\n",
              " 'CantStopPoppin',\n",
              " 'JinaJoe',\n",
              " 'somnifacientsawyer',\n",
              " 'KuduIO',\n",
              " 'Kayozuki',\n",
              " 'jafreese98',\n",
              " 'SonictheManhog',\n",
              " 'ggstardust69',\n",
              " 'Antiliani',\n",
              " 'Jellyrollrider',\n",
              " 'joyousjoyness',\n",
              " 'JordanCarter77',\n",
              " 'Cloud9Shopper',\n",
              " 'Driscoll17',\n",
              " 'chrisdh79',\n",
              " 'n0x0ne',\n",
              " 'Gamepwn',\n",
              " 'ArsenalWillBeBack',\n",
              " 'Caiyul',\n",
              " 'Cannibaloxfords10',\n",
              " 'ninjyte',\n",
              " 'stealthyfrog',\n",
              " 'clemaneuverers',\n",
              " 'twistedlogicx',\n",
              " 'anon7935678',\n",
              " 'Mimi108',\n",
              " 'wwwiphala',\n",
              " 'patiencetruth',\n",
              " 'Iwannadrinkthebleach',\n",
              " 'memelord_harsh',\n",
              " 'Evoqu_',\n",
              " 'Facts-Over-Opinion',\n",
              " 'zsreport',\n",
              " 'Ravenbtw',\n",
              " 'Ninten-Doh',\n",
              " 'ObnoxiousOldBastard',\n",
              " 'CeballaLight7',\n",
              " 'mendelevium34',\n",
              " 'dustin_ginsberg',\n",
              " 'inspiration_capsule',\n",
              " 'Typoqueen00',\n",
              " 'xanaxcervix',\n",
              " 'cyrusasu',\n",
              " 'Gambit08',\n",
              " 'Hyorennn',\n",
              " 'Hafomeng',\n",
              " 'orgiasticfuture',\n",
              " 'boomerpro',\n",
              " 'PMSlimeKing',\n",
              " 'ravedog',\n",
              " 'ghooosst',\n",
              " 'madman320',\n",
              " 'Defie-LOH-Gic',\n",
              " 'shookqueen',\n",
              " 'habichuelacondulce',\n",
              " 'tttrrruuu',\n",
              " 'kimcheefarts',\n",
              " 'My_Memes_Will_Cure_U',\n",
              " 'The1stCitizenOfTheIn',\n",
              " 'i[***]y[***]n',\n",
              " '[***]eandethercreature',\n",
              " 'bear-rah',\n",
              " 'sailorjupiter28titan',\n",
              " 'farhan9835',\n",
              " 'arbitopi',\n",
              " 'ohnoh18',\n",
              " 'pandabeardnm',\n",
              " 'WorkTomorrow',\n",
              " 'CriminalAir',\n",
              " 'justcasty',\n",
              " 'UpNDownCan',\n",
              " 'Ch33rn0',\n",
              " 'jaymar01',\n",
              " 'opi_oid',\n",
              " 'suitabledz',\n",
              " 'dunphish64',\n",
              " 'kogeliz',\n",
              " 'Razerer92',\n",
              " 'Aerobics111',\n",
              " 'Havvocck2',\n",
              " 'Allstarhit',\n",
              " 'axolotl_peyotl',\n",
              " 'casualbadideas',\n",
              " 'DaFunkJunkie',\n",
              " 'MakeItRainSheckels',\n",
              " 'OnHolidayHere',\n",
              " 'chovy',\n",
              " 'Arkam_slayer66',\n",
              " 'BlanketMage',\n",
              " 'henryhuff',\n",
              " 'hildebrand_rarity',\n",
              " 'Waldonville',\n",
              " 'invisiblefaction2',\n",
              " 'pranksterdankster',\n",
              " 'Saibasaurus',\n",
              " 'MisterT12',\n",
              " 'allofusahab',\n",
              " 'pathogenalpha',\n",
              " 'mythrowawaybabies',\n",
              " 'SleepNowMyThrowaway',\n",
              " 'Fuckyousantorum',\n",
              " 'Ragnarokcometh',\n",
              " 'linaching',\n",
              " 'OldFashionedJizz',\n",
              " 'jsinkwitz',\n",
              " 'professorMob',\n",
              " 'TheIlluminatiEatPoo',\n",
              " 'LateData',\n",
              " 'TaurusII',\n",
              " 'cosmicprank',\n",
              " 'meektakeL',\n",
              " 'Stoaticor',\n",
              " 'scamaltert',\n",
              " 'meister2a',\n",
              " 'wolfsog23',\n",
              " 'RabidNemo',\n",
              " 'txiao007',\n",
              " 'stem12345679',\n",
              " 'Willis8604',\n",
              " 'r[***]og',\n",
              " 'KitKatHasClaws',\n",
              " 'jo_jo_nyeb',\n",
              " 'avivi_',\n",
              " 'conam25',\n",
              " 'grantalx',\n",
              " 'darkoms666',\n",
              " 'TobySomething',\n",
              " 'lilmcfuggin',\n",
              " 'sbpotdbot',\n",
              " 'mvpeast',\n",
              " 'politicalthrow99',\n",
              " 'AlitaBattlePringleTM',\n",
              " 'geoxol',\n",
              " 'Kwindecent_exposure',\n",
              " 'BanDerUh',\n",
              " 'Al-Andalusia',\n",
              " '-eDgAR-',\n",
              " 'SAMUEL_L_JACKASS',\n",
              " 'Sleegan',\n",
              " 'invertedparado[***]',\n",
              " 'notpreposterous',\n",
              " 'DaRandomGitty2',\n",
              " 'Romano16',\n",
              " 'TurtleFacts72',\n",
              " 'bobby_triple',\n",
              " 'schuey_08',\n",
              " 'stargem5',\n",
              " 'praaany',\n",
              " 'kevinmrr',\n",
              " 'Steven1958',\n",
              " 'puppuli',\n",
              " 'callmebaiken',\n",
              " 'freq-ee',\n",
              " 'vizard673',\n",
              " 'MrEmerald2006',\n",
              " 'Jokengonzo',\n",
              " 'GlobalCitizen12345',\n",
              " 'Hellollie',\n",
              " 'Jkid',\n",
              " 'truthwillout777',\n",
              " 'ofcabbagesandkings14',\n",
              " 'Jumido730',\n",
              " 'genericwan',\n",
              " 'Dystopiannie',\n",
              " 'soldio101',\n",
              " 'kirby__000',\n",
              " 'Fickkissen',\n",
              " 'Dark_Diggler',\n",
              " 'NotsoPG',\n",
              " 'discocrisco',\n",
              " 'dodgyb',\n",
              " 'samarai4444',\n",
              " 'Lshim',\n",
              " 'AnakinWayneII',\n",
              " 'Defie-LOH-Giq',\n",
              " 'SuperJohnny25',\n",
              " 'Majnum',\n",
              " 'Joe_Tazuna',\n",
              " 'Iarguewith[***]s',\n",
              " 'whk1992',\n",
              " 'AutoModerator',\n",
              " 'Not4Reel',\n",
              " 'Deshes011',\n",
              " 'nightcloudsky',\n",
              " 'asad1ali2',\n",
              " '61539',\n",
              " 'akarim5847',\n",
              " 'theitguyforever',\n",
              " 'MaiSenpaii2',\n",
              " 'Gayfetus',\n",
              " 'LuanderSchwarze',\n",
              " 'DroppingPing',\n",
              " 'Lost_Distribution546',\n",
              " 'walkinman19',\n",
              " 'Kinmuan',\n",
              " 'caststonegl[***]home',\n",
              " 'jetvacjesse',\n",
              " 'Altruistic_Astronaut',\n",
              " 'relevantlife',\n",
              " 'rebooted_life_42',\n",
              " 'imagepoem',\n",
              " 'Giraffecaster',\n",
              " 'DeWallenVanWimKok',\n",
              " 'AdamCannon',\n",
              " 'TheAtheistArab87',\n",
              " 'SufficientHistorian6',\n",
              " 'bored_in_NE',\n",
              " 'Livelikethelotus',\n",
              " 'Mahomeboy_',\n",
              " 'Magro18',\n",
              " 'Good-N-2020',\n",
              " 'Cross_Ange',\n",
              " 'Shi[***]ihost',\n",
              " 'ryanaire5',\n",
              " 'karmagheden',\n",
              " 'oliver_21',\n",
              " 'PropagandaDebunker',\n",
              " 'Pessimist2020',\n",
              " 'mostrandomguy',\n",
              " 'seamslegit',\n",
              " 'north0east',\n",
              " 'swingadmin',\n",
              " 'signed7',\n",
              " 'masked-n-anonymous',\n",
              " 'Thedepressionoftrees',\n",
              " 'Gonzo_B',\n",
              " 'Morihando',\n",
              " 'KatieAllTheTime',\n",
              " 'DoremusJessup',\n",
              " '11_gop',\n",
              " 'DesignerAttitude98',\n",
              " 'dat1guy420',\n",
              " 'kadirba68',\n",
              " 'seanspeaks77',\n",
              " 'NYLaw',\n",
              " 'ThegrayD',\n",
              " 'pikcoolski',\n",
              " 'ratioetlogicae',\n",
              " 'StardustRedditor',\n",
              " 'enterprisevalue',\n",
              " '5nordehacedod',\n",
              " 'werdmouf',\n",
              " '[***]reader',\n",
              " 'VeganSamura1',\n",
              " 'johnslegers',\n",
              " 'BearsNecessity',\n",
              " 'marinatingpandemic',\n",
              " 'censor_this_commie',\n",
              " 'Gari_305',\n",
              " 'Kreygasm2233',\n",
              " 'invertedparadX',\n",
              " 'ElectronicFudge5',\n",
              " 'apocalypticalley',\n",
              " 'bemani4u',\n",
              " 'One_Curious_Jay',\n",
              " 'Zhana-Aul',\n",
              " 'kirose',\n",
              " 'wesoly17',\n",
              " 'hunt4redglocktober',\n",
              " 'bobbyw24',\n",
              " 'TitoHernandez',\n",
              " 'GuthixIsBalance',\n",
              " 'jcepiano',\n",
              " 'lilstinky[***]',\n",
              " 'TheGamerDanYT',\n",
              " 'litbacod4',\n",
              " 'Maximus_Ballsackus',\n",
              " 'hash0t0',\n",
              " 'L-Z-1',\n",
              " 'KSCGA',\n",
              " 'I_am_a_[***]_to_ants',\n",
              " 'wTone_',\n",
              " 'Orisaaaaa',\n",
              " 'DoodlebobIsMy[***]',\n",
              " 'Normiesreeee69',\n",
              " 'fallenkeith2018',\n",
              " 'kanye5150',\n",
              " 'TAKEitTOrCIRCLEJERK',\n",
              " 'redmambo_no6',\n",
              " 'allicat83',\n",
              " 'ProudAmerican_MO',\n",
              " 'BrunoofBrazil',\n",
              " 'alwayswashere',\n",
              " 'Raaapid',\n",
              " 'notinferno',\n",
              " 'MrRoxx',\n",
              " 'Dajakesta0624',\n",
              " 'kaze_ni_naru',\n",
              " 'xItacolomix',\n",
              " 'gameskull11',\n",
              " 'Scoopide[***]',\n",
              " 'ostrichesarenotreal',\n",
              " 'tacolben',\n",
              " 'Playaguy',\n",
              " 'Xeelee1123',\n",
              " 'lukeu42',\n",
              " 'Ramy_',\n",
              " 'harushiga',\n",
              " 'MyPenisIsALesbian',\n",
              " 'jaimelancaster',\n",
              " 'Humble_Award_4873',\n",
              " '0ldManFrank',\n",
              " 'Sleepy_Bob_Ross',\n",
              " 'Vasallo7G',\n",
              " 'Smileitsolga',\n",
              " 'xrangegod1',\n",
              " 'koavf',\n",
              " 'blacked_lover',\n",
              " 'kevinowdziej',\n",
              " 'HeftyArt4',\n",
              " 'miketheman0506',\n",
              " 'kapetankuka',\n",
              " 'CapitalCourse',\n",
              " 'noahsurvived',\n",
              " 'EyeWikeWocketz',\n",
              " 'ZOEXofficial',\n",
              " 'mouthofreason']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrl0kq09dxrp"
      },
      "source": [
        "### P1.2.2 - Distribution of posts per weekday (5 marks)\n",
        "\n",
        "Find the percentage of posts that were posted in each weekday (Monday, Tuesday, etc.). You can use an external calendar or you can use any functionality for dealing with dates available in pandas. \n",
        "\n",
        "**What to implement:** A function `get_weekday_post_distribution(df)` that takes as input the original dataframe and returns a dictionary of the form (the values are made up):\n",
        "\n",
        "```\n",
        "{'Monday': '14%',\n",
        "'Tuesday': '23%', \n",
        "...\n",
        "}\n",
        "```\n",
        "\n",
        "Note that you must only return two decimals, and you must include the percentage sign in the output dictionary. \n",
        "\n",
        "Note that in dictionaries order is not preserved, so the order in which it gets printed will not matter. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weekday_post_distribution(df):\n",
        "  # your answer here\n",
        "  date_to_weekdays = []\n",
        "  for i, row in df.iterrows():\n",
        "    date_to_weekdays.append(datetime.strptime(row['posted_at'], '%Y-%m-%d %H:%M:%S').strftime('%A'))\n",
        "\n",
        "  \n",
        "  total_number_of_rows = df.shape[0]\n",
        "  return {\n",
        "      \"Monday\":f\"{int(round(date_to_weekdays.count('Monday')/total_number_of_rows,2)*100)}%\",\n",
        "      \"Tuesday\":f\"{int(round(date_to_weekdays.count('Tuesday')/total_number_of_rows,2)*100)}%\",\n",
        "      \"Wednesday\":f\"{int(round(date_to_weekdays.count('Wednesday')/total_number_of_rows,2)*100)}%\",\n",
        "      \"Thursday\":f\"{int(round(date_to_weekdays.count('Thursday')/total_number_of_rows,2)*100)}%\",\n",
        "      \"Friday\":f\"{int(round(date_to_weekdays.count('Friday')/total_number_of_rows,2)*100)}%\",\n",
        "      \"Saturday\":f\"{int(round(date_to_weekdays.count('Saturday')/total_number_of_rows,2)*100)}%\",\n",
        "      \"Sunday\":f\"{int(round(date_to_weekdays.count('Sunday')/total_number_of_rows,2)*100)}%\"\n",
        "  }"
      ],
      "metadata": {
        "id": "Aj_2ss9jy9WC"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_weekday_post_distribution(df)"
      ],
      "metadata": {
        "id": "5FUpOzgN1t3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2ba33d3-ea1a-470a-a64b-a448d499c4e2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Monday': '14%',\n",
              " 'Tuesday': '15%',\n",
              " 'Wednesday': '15%',\n",
              " 'Thursday': '15%',\n",
              " 'Friday': '15%',\n",
              " 'Saturday': '14%',\n",
              " 'Sunday': '13%'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVj1WikSUPjO"
      },
      "source": [
        "### P1.2.3 - The 100 most passionate redditors (7 marks)\n",
        "\n",
        "We would like to know which are the 100 redditors (`author` column) that are most passionate. We will measure this by checking, for each redditor, the ratio at which they use adjectives. This ratio will be computed by dividing number of adjectives by the total number of words each redditor used. The analysis will only consider redditors that have written at least 1000 words.\n",
        "\n",
        "**What to implement:** A function called `get_passionate_redditors(df)` that takes as input the original dataframe and returns a list of the top 100 redditors (authors) by the ratio at which they use adjectives considering both the `title` and `selftext` columns. The returned list should be a list of tuples, where each inner tuple has two elements: the redditor (author) name, and the ratio of adjectives they used. The returned list should be sorted by adjective ratio in descending order (highest first). Only redditors that wrote more than 1000 words should be considered. You should use `nltk`'s `word_tokenize` and `pos_tag` functions to tokenize and find adjectives. You do not need to do any preprocessing like stopword removal, lemmatization or stemming."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_passionate_redditors(df):\n",
        "  # your answer here"
      ],
      "metadata": {
        "id": "yjfpDjS2oPzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_passionate_redditors(df)"
      ],
      "metadata": {
        "id": "7ddl-35Trg2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## P1.3 Ethics (10 marks)"
      ],
      "metadata": {
        "id": "jQKJ4zc9UyOc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsAF9jpblJLp"
      },
      "source": [
        "Imagine you are **the head of a data mining company** that needs to use the insights gained in this assignment to scan social media for covid-related content, and automatically flag it as conspiracy or not conspiracy (for example, for hiding potentially harmful tweets or Facebook posts). Some\n",
        "information about the project and the team:\n",
        "\n",
        "- Your client is a political party concerned about misinformation.\n",
        "- The project requires mining Facebook, Reddit and Instagram data.\n",
        "- The team consists of Joe, an American mathematician who just finished college; Fei, a senior software engineer from China; and Francisco, a data scientist from Spain.\n",
        "\n",
        "Reflect on the impact of exploiting data science for such an application. You should map your discussion to one of the five actions outlined in the UKâ€™s Data Ethics Framework.\n",
        "\n",
        "Your answer should address the following:\n",
        "- Identify the action in which your project is the weakest.\n",
        "- Then, justify your choice by critically analyzing the three key principles for that action outlined\n",
        "in the Framework, namely transparency, accountability and fairness.\n",
        "- Finally, you should propose one solution that explicitly addresses one point related to one of these three principles, reflecting on how your solution would improve the data cycle in this particular use case.\n",
        "\n",
        "Your answer should be between 500 and 700 words. **You are strongly encouraged to follow a scholarly approach, e.g., with references to peer reviewed publications. References do not count towards the word limit**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YJQSO8Amuea"
      },
      "source": [
        "---\n",
        "\n",
        "Your answer here"
      ]
    }
  ]
}