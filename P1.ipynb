{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CMT309 - Computational Data Science - Data Science Portfolio**"
      ],
      "metadata": {
        "id": "t_hYgOgUzIwe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCAPWR2s3nzA"
      },
      "source": [
        "# Part 1 - Text Data Analysis (45 marks)\n",
        "\n",
        "In this question you will write Python code for processing, analyzing and understanding the social network **Reddit** (www.reddit.com). Reddit is a platform that allows users to upload posts and comment on them, and is divided in _subreddits_, often covering specific themes or areas of interest (for example, [world news](https://www.reddit.com/r/worldnews/), [ukpolitics](https://www.reddit.com/r/ukpolitics/) or [nintendo](https://www.reddit.com/r/nintendo)). You are provided with a subset of Reddit with posts from Covid-related subreddits (e.g., _CoronavirusUK_ or _NoNewNormal_), as well as randomly selected subreddits (e.g., _donaldtrump_ or _razer_).\n",
        "\n",
        "The `csv` dataset you are provided contains one row per post, and has information about three entities: **posts**, **users** and **subreddits**. The column names are self-explanatory: columns starting with the prefix `user_` describe users, those starting with the prefix `subr_` describe subreddits, the `subreddit` column is the subreddit name, and the rest of the columns are post attributes (`author`, `posted_at`, `title` and post text - the `selftext` column-, number of comments - `num_comments`, `score`, etc.).\n",
        "\n",
        "In this exercise, you are asked to perform a number of operations to gain insights from the data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22070780"
      ],
      "metadata": {
        "id": "gAcSdyt1QMQy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pm74v1u4d6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "828096f9-c281-46ee-bd78-eb8e9e8db8b8"
      },
      "source": [
        "#We import all the neccessary libraries\n",
        "# suggested imports\n",
        "import pandas as pd\n",
        "from nltk.tag import pos_tag\n",
        "import re\n",
        "from collections import defaultdict,Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "tqdm.pandas()\n",
        "from ast import literal_eval\n",
        "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## P1.0) Suggested/Required Imports"
      ],
      "metadata": {
        "id": "jlssd-CQOOXE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfNsDQ253nzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25498547-b882-427e-bed9-fcaa9b0e3365"
      },
      "source": [
        "from urllib import request\n",
        "import pandas as pd\n",
        "module_url = f\"https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_22.csv\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  a = f.read()\n",
        "  outf.write(a.decode('utf-8'))\n",
        "df = pd.read_csv('data_portfolio_22.csv')\n",
        "# this fills empty cells with empty strings\n",
        "df = df.fillna('')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_22.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CNfbxg2X3nzK",
        "outputId": "d2b9cdf3-2c68-463f-affb-343eb31954f8"
      },
      "source": [
        "#We display the dataset to visualize it and see the various rows and columns\n",
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           author            posted_at  num_comments  score  \\\n",
              "0      -Howitzer-  2020-08-17 20:26:04            19      1   \n",
              "1      -Howitzer-  2020-07-06 17:01:48             1      3   \n",
              "2      -Howitzer-  2020-09-09 02:29:02             3      1   \n",
              "3      -Howitzer-  2020-06-23 23:02:39             2      1   \n",
              "4      -Howitzer-  2020-08-07 04:13:53            32    622   \n",
              "...           ...                  ...           ...    ...   \n",
              "19935     zqrwiel  2020-07-23 16:39:15            11    246   \n",
              "19936     zqrwiel  2020-12-15 11:25:07            39      1   \n",
              "19937     zqrwiel  2020-12-27 13:57:49            15      1   \n",
              "19938     zqrwiel  2020-12-29 12:07:10             6      1   \n",
              "19939     zqrwiel  2021-01-21 16:47:13            23      1   \n",
              "\n",
              "                                                selftext subr_created_at  \\\n",
              "0                                                             2009-04-29   \n",
              "1                                                             2009-04-29   \n",
              "2                                                             2009-04-29   \n",
              "3                                                             2009-04-29   \n",
              "4                                                             2009-04-29   \n",
              "...                                                  ...             ...   \n",
              "19935                                                         2009-04-13   \n",
              "19936  Then I think we might get 18 songs, outro usua...      2009-04-13   \n",
              "19937  He has 25songs to perform plus the additional ...      2009-04-13   \n",
              "19938     I got goose[***]ps just by thinking about it ðŸ˜¬      2009-04-13   \n",
              "19939                                                         2009-04-13   \n",
              "\n",
              "                                        subr_description  \\\n",
              "0                           Subreddit about Donald Trump   \n",
              "1                           Subreddit about Donald Trump   \n",
              "2                           Subreddit about Donald Trump   \n",
              "3                           Subreddit about Donald Trump   \n",
              "4                           Subreddit about Donald Trump   \n",
              "...                                                  ...   \n",
              "19935  A subreddit dedicated to the discussion of hip...   \n",
              "19936  A subreddit dedicated to the discussion of hip...   \n",
              "19937  A subreddit dedicated to the discussion of hip...   \n",
              "19938  A subreddit dedicated to the discussion of hip...   \n",
              "19939  A subreddit dedicated to the discussion of hip...   \n",
              "\n",
              "                                           subr_faved_by  subr_numb_members  \\\n",
              "0      ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "1      ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "2      ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "3      ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "4      ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "...                                                  ...                ...   \n",
              "19935  ['solex125', 'redreddington22', 'HibikiSS', 'k...               8740   \n",
              "19936  ['solex125', 'redreddington22', 'HibikiSS', 'k...               8740   \n",
              "19937  ['solex125', 'redreddington22', 'HibikiSS', 'k...               8740   \n",
              "19938  ['solex125', 'redreddington22', 'HibikiSS', 'k...               8740   \n",
              "19939  ['solex125', 'redreddington22', 'HibikiSS', 'k...               8740   \n",
              "\n",
              "       subr_numb_posts     subreddit  \\\n",
              "0               796986   donaldtrump   \n",
              "1               796986   donaldtrump   \n",
              "2               796986   donaldtrump   \n",
              "3               796986   donaldtrump   \n",
              "4               796986   donaldtrump   \n",
              "...                ...           ...   \n",
              "19935           630857  playboicarti   \n",
              "19936           630857  playboicarti   \n",
              "19937           630857  playboicarti   \n",
              "19938           630857  playboicarti   \n",
              "19939           630857  playboicarti   \n",
              "\n",
              "                                                   title  \\\n",
              "0      BREAKING: Trump to begin hiding in mailboxes t...   \n",
              "1                                    Joe Biden's America   \n",
              "2      4 more years and we can erase his legacy for g...   \n",
              "3      Revelation 9:6 [Transhumanism: The New Religio...   \n",
              "4                                         LOOK HERE, FAT   \n",
              "...                                                  ...   \n",
              "19935                                          carti why   \n",
              "19936                           If uzi on track 3 and 16   \n",
              "19937          Man cartiâ€™s concerts are gonna be long af   \n",
              "19938  Canâ€™t wait to see Carti going full rage mode o...   \n",
              "19939           [OFFTOPIC] have you seen that new LV? ðŸ˜‚ðŸ’€   \n",
              "\n",
              "       total_awards_received  upvote_ratio  user_num_posts user_registered_at  \\\n",
              "0                          0          1.00            4661         2012-11-09   \n",
              "1                          0          0.67            4661         2012-11-09   \n",
              "2                          0          1.00            4661         2012-11-09   \n",
              "3                          0          1.00            4661         2012-11-09   \n",
              "4                          0          0.88            4661         2012-11-09   \n",
              "...                      ...           ...             ...                ...   \n",
              "19935                      0          1.00            1883         2014-02-12   \n",
              "19936                      0          1.00            1883         2014-02-12   \n",
              "19937                      0          1.00            1883         2014-02-12   \n",
              "19938                      0          1.00            1883         2014-02-12   \n",
              "19939                      0          1.00            1883         2014-02-12   \n",
              "\n",
              "       user_upvote_ratio  \n",
              "0              -0.658599  \n",
              "1              -0.658599  \n",
              "2              -0.658599  \n",
              "3              -0.658599  \n",
              "4              -0.658599  \n",
              "...                  ...  \n",
              "19935           0.861626  \n",
              "19936           0.861626  \n",
              "19937           0.861626  \n",
              "19938           0.861626  \n",
              "19939           0.861626  \n",
              "\n",
              "[19940 rows x 17 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d4b7f11e-db90-4d69-8e6c-8c3c2ac50096\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>posted_at</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>score</th>\n",
              "      <th>selftext</th>\n",
              "      <th>subr_created_at</th>\n",
              "      <th>subr_description</th>\n",
              "      <th>subr_faved_by</th>\n",
              "      <th>subr_numb_members</th>\n",
              "      <th>subr_numb_posts</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>title</th>\n",
              "      <th>total_awards_received</th>\n",
              "      <th>upvote_ratio</th>\n",
              "      <th>user_num_posts</th>\n",
              "      <th>user_registered_at</th>\n",
              "      <th>user_upvote_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-08-17 20:26:04</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>BREAKING: Trump to begin hiding in mailboxes t...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-07-06 17:01:48</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>Joe Biden's America</td>\n",
              "      <td>0</td>\n",
              "      <td>0.67</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-09-09 02:29:02</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>4 more years and we can erase his legacy for g...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-06-23 23:02:39</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>Revelation 9:6 [Transhumanism: The New Religio...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-08-07 04:13:53</td>\n",
              "      <td>32</td>\n",
              "      <td>622</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>LOOK HERE, FAT</td>\n",
              "      <td>0</td>\n",
              "      <td>0.88</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19935</th>\n",
              "      <td>zqrwiel</td>\n",
              "      <td>2020-07-23 16:39:15</td>\n",
              "      <td>11</td>\n",
              "      <td>246</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-13</td>\n",
              "      <td>A subreddit dedicated to the discussion of hip...</td>\n",
              "      <td>['solex125', 'redreddington22', 'HibikiSS', 'k...</td>\n",
              "      <td>8740</td>\n",
              "      <td>630857</td>\n",
              "      <td>playboicarti</td>\n",
              "      <td>carti why</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1883</td>\n",
              "      <td>2014-02-12</td>\n",
              "      <td>0.861626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19936</th>\n",
              "      <td>zqrwiel</td>\n",
              "      <td>2020-12-15 11:25:07</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>Then I think we might get 18 songs, outro usua...</td>\n",
              "      <td>2009-04-13</td>\n",
              "      <td>A subreddit dedicated to the discussion of hip...</td>\n",
              "      <td>['solex125', 'redreddington22', 'HibikiSS', 'k...</td>\n",
              "      <td>8740</td>\n",
              "      <td>630857</td>\n",
              "      <td>playboicarti</td>\n",
              "      <td>If uzi on track 3 and 16</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1883</td>\n",
              "      <td>2014-02-12</td>\n",
              "      <td>0.861626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19937</th>\n",
              "      <td>zqrwiel</td>\n",
              "      <td>2020-12-27 13:57:49</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>He has 25songs to perform plus the additional ...</td>\n",
              "      <td>2009-04-13</td>\n",
              "      <td>A subreddit dedicated to the discussion of hip...</td>\n",
              "      <td>['solex125', 'redreddington22', 'HibikiSS', 'k...</td>\n",
              "      <td>8740</td>\n",
              "      <td>630857</td>\n",
              "      <td>playboicarti</td>\n",
              "      <td>Man cartiâ€™s concerts are gonna be long af</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1883</td>\n",
              "      <td>2014-02-12</td>\n",
              "      <td>0.861626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19938</th>\n",
              "      <td>zqrwiel</td>\n",
              "      <td>2020-12-29 12:07:10</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>I got goose[***]ps just by thinking about it ðŸ˜¬</td>\n",
              "      <td>2009-04-13</td>\n",
              "      <td>A subreddit dedicated to the discussion of hip...</td>\n",
              "      <td>['solex125', 'redreddington22', 'HibikiSS', 'k...</td>\n",
              "      <td>8740</td>\n",
              "      <td>630857</td>\n",
              "      <td>playboicarti</td>\n",
              "      <td>Canâ€™t wait to see Carti going full rage mode o...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1883</td>\n",
              "      <td>2014-02-12</td>\n",
              "      <td>0.861626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19939</th>\n",
              "      <td>zqrwiel</td>\n",
              "      <td>2021-01-21 16:47:13</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-13</td>\n",
              "      <td>A subreddit dedicated to the discussion of hip...</td>\n",
              "      <td>['solex125', 'redreddington22', 'HibikiSS', 'k...</td>\n",
              "      <td>8740</td>\n",
              "      <td>630857</td>\n",
              "      <td>playboicarti</td>\n",
              "      <td>[OFFTOPIC] have you seen that new LV? ðŸ˜‚ðŸ’€</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1883</td>\n",
              "      <td>2014-02-12</td>\n",
              "      <td>0.861626</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19940 rows Ã— 17 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d4b7f11e-db90-4d69-8e6c-8c3c2ac50096')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d4b7f11e-db90-4d69-8e6c-8c3c2ac50096 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d4b7f11e-db90-4d69-8e6c-8c3c2ac50096');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyQyR27z48nr"
      },
      "source": [
        "## P1.1 - Text data processing (20 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfpA4Z5ADdok"
      },
      "source": [
        "### P1.1.1 - Offensive authors per subreddit (5 marks)\n",
        "\n",
        "As you will see, the dataset contains a lot of strings of the form `[***]`. These have been used to mask (or remove) swearwords to make it less offensive. We are interested in finding those users that have posted at least one swearword in each subreddit. We do this by counting occurrences of the `[***]` string in the `selftext` column (we can assume that an occurrence of `[***]` equals a swearword in the original dataset).\n",
        "\n",
        "**What to implement:** A function `offensive_authors(df)` that takes as input the original dataframe and returns a dataframe of the form below, where each row contains authors that posted at least one swearword in the corresponding subreddit.\n",
        "\n",
        "```\n",
        "subreddit\tauthor\n",
        "0\t40kLore\tCross_Ange\n",
        "1\t40kLore\tDaRandomGitty2\n",
        "2\t40kLore\tEMB1981\n",
        "3\t40kLore\tEvoxrus_XV\n",
        "4\t40kLore\tGrtrshop\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def offensive_authors(df):\n",
        "  # we print all the offensive authors who used at least one swearword\n",
        "  return df[df['selftext'].str.contains(\"[***]\")][['subreddit','author']]\n",
        "  "
      ],
      "metadata": {
        "id": "9oby6Xbz59F0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offensive_authors(df)"
      ],
      "metadata": {
        "id": "FdiKSgWa9dKO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "31f8c477-ceb6-4f6d-ce55-cf35b8f30c3c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          subreddit              author\n",
              "44       conspiracy            0naptoon\n",
              "47       conspiracy            0naptoon\n",
              "51       conspiracy  10100011a10100011a\n",
              "76       conspiracy         13followsMe\n",
              "91       conspiracy     2012ronpaul2012\n",
              "...             ...                 ...\n",
              "19841  playboicarti          yoda_[***]\n",
              "19849  playboicarti          yoda_[***]\n",
              "19930  playboicarti             zqrwiel\n",
              "19933  playboicarti             zqrwiel\n",
              "19938  playboicarti             zqrwiel\n",
              "\n",
              "[1200 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e312f723-ad33-4c06-b1d6-aa44cb249c38\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subreddit</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>conspiracy</td>\n",
              "      <td>0naptoon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>conspiracy</td>\n",
              "      <td>0naptoon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>conspiracy</td>\n",
              "      <td>10100011a10100011a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>conspiracy</td>\n",
              "      <td>13followsMe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>conspiracy</td>\n",
              "      <td>2012ronpaul2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19841</th>\n",
              "      <td>playboicarti</td>\n",
              "      <td>yoda_[***]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19849</th>\n",
              "      <td>playboicarti</td>\n",
              "      <td>yoda_[***]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19930</th>\n",
              "      <td>playboicarti</td>\n",
              "      <td>zqrwiel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19933</th>\n",
              "      <td>playboicarti</td>\n",
              "      <td>zqrwiel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19938</th>\n",
              "      <td>playboicarti</td>\n",
              "      <td>zqrwiel</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1200 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e312f723-ad33-4c06-b1d6-aa44cb249c38')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e312f723-ad33-4c06-b1d6-aa44cb249c38 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e312f723-ad33-4c06-b1d6-aa44cb249c38');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhZ3u5aS3rrm"
      },
      "source": [
        "### P1.1.2 - Most common trigrams per subreddit (15 marks)\n",
        "\n",
        "We are interested in learning about _the ten most frequent trigrams_ (a [trigram](https://en.wikipedia.org/wiki/Trigram) is a sequence of three consecutive words) in each subreddit's content. You must compute these trigrams on both the `selftext` and `title` columns. Your task is to generate a Python dictionary of the form:\n",
        "\n",
        "```\n",
        "{subreddit1: [(trigram1, freq1), (trigram2, freq2), ... , (trigram3, freq10)],\n",
        "subreddit1: [(trigram1, freq1), (trigram2, freq2), ... , (trigram3, freq10)],\n",
        "...\n",
        "subreddit63: [(trigram1, freq1), (trigram2, freq2), ... , (trigram3, freq10)],}\n",
        "```\n",
        "\n",
        "That is, for each subreddit, the 10 most frequent trigrams and their frequency, stored in a list of tuples. Each trigram will be stored also as a tuple containing 3 strings.\n",
        "\n",
        "**What to implement**: A function `get_tris(df, stopwords_list, punctuation_list)` that will take as input the original dataframe, a list of stopwords and a list of punctuation signs (e.g., `?` or `!`), and will return a python dictionary with the above format. Your function must implement the following steps in order:\n",
        "\n",
        "- (**1 mark**) Create a new dataframe called `newdf` with only `subreddit`, `title` and `selftext` columns.\n",
        "- (**1 mark**) Add a new column to `newdf` called `full_text`, which will contain `title` and `selftext` concatenated with the string `.` (a full stop) followed by a space. That, is `A simple title` and `This is a text body` would be `A simple title. This is a text body`.\n",
        "- (**1 mark**) Remove all occurrences of the following strings from `full_text`. You must do this without creating a new column:\n",
        "  - `[***]`\n",
        "  - `&amp;`\n",
        "  - `&gt;`\n",
        "  - `https`\n",
        "- (**1 mark**) You must also remove all occurrences of at least three consecutive hyphens, for example, you should remove strings like `---`, `----`, `-----`, etc., but not `--` and not `-`.\n",
        "- (**1 mark**) Tokenize the contents of the `full_text` column after lower casing (removing all capitalization). You should use the `word_tokenize` function in `nltk`. Add the results to a new column called `full_text_tokenized`.\n",
        "- (**2 mark**) Remove all tokens that are either stopwords or punctuation from `full_text_tokenized` and store the results in a new column called `full_text_tokenized_clean`. _See Note 1_.\n",
        "- (**2 marks**) Create a new dataframe called `adf` (which will stand for _aggregated dataframe_), which will have one row per subreddit (i.e., 63 rows), and will have two columns: `subreddit` (the subreddit name), and `all_words`, which will be a big list with all the words that belong to that subreddit as extracted from the `full_text_tokenized_clean`.\n",
        "- (**3 marks**) Obtain trigram counts, which will be stored in a dictionary where each `key` will be a trigram (a `tuple` containing 3 consecutive tokens), and each `value` will be their overall frequency in that subreddit. You are  encouraged to use functions from the `nltk` package, although you can choose any approach to solve this part.\n",
        "- (**3 marks**) Finally, use the information you have in `adf` for generating the desired dictionary, and return it. _See Note 2_.\n",
        "\n",
        "Note 1. You can obtain stopwords and punctuation as follows.\n",
        "- Stopwords: \n",
        "```\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('english')\n",
        "```\n",
        "- Punctuation:\n",
        "```\n",
        "import string\n",
        "punctuation = list(string.punctuation)\n",
        "```\n",
        "\n",
        "Note 2. You do not have to apply an additional ordering when there are several trigrams with the same frequency."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports here for extra clarity\n",
        "from nltk.corpus import stopwords as sw\n",
        "import string\n",
        "import warnings\n",
        "def get_tris(df, stopwords_list, punctuation_list):\n",
        "  # 1 MARK - create new df with only relevant columns\n",
        "  newdf = df[['subreddit','title', 'selftext']]\n",
        "  \n",
        "  # 1 MARK - concatenate title and selftext\n",
        "  # 1 MARK for string replacement\n",
        "  newdf['full_text'] = df.apply(lambda row: f\"{row.title}. {row.selftext}\", axis=1)\n",
        "  \n",
        "  # 1 MARK for regex replacement - remove the strings \"[***]\", \"&amp;\", \"&gt;\" and \"https\", also at least three consecutive dashes\n",
        "  prohibitedWords = ['[***]', '&amp;', '&gt;', 'https']\n",
        "  regex1 = re.compile('|'.join(map(re.escape, prohibitedWords)))\n",
        "  newdf['full_text'] = newdf.apply(lambda row: regex1.sub(\"\", row.full_text), axis=1)\n",
        "  newdf['full_text'] = newdf.apply(lambda row: re.sub(\"/-{3,}/\" ,\"\", row.full_text), axis=1)\n",
        "\n",
        "  # 1 MARK - lower case, tokenize, and add result to full_text_tokenize\n",
        "  newdf['full_text_tokenized'] = newdf.apply(lambda row: word_tokenize(row.full_text.lower()), axis=1)\n",
        "  \n",
        "  # 2 MARKS - clean the full_text_tokenized column by iterating over each word and discarding if it's either a stopword or punctuation\n",
        "  newdf['full_text_tokenized_clean'] = newdf.apply(lambda row: [x for x in row.full_text_tokenized if x not in stopwords_list + punctuation_list], axis=1)\n",
        "  \n",
        "  # 2 MARKS - create new aggregated dataframe by concatenating all full_text_tokenized_clean values - rename columns as requested\n",
        "  adf = newdf.groupby('subreddit').agg({'full_text_tokenized_clean': 'sum'})\n",
        "  \n",
        "  # 3 MARKS - create new Series object by piping nltk's FreqDist and trigrams functions into all_words\n",
        "  adf['all_words'] = adf.apply(lambda row: nltk.FreqDist(nltk.trigrams(row.full_text_tokenized_clean)).most_common(10), axis=1)\n",
        "  \n",
        "  # 3 MARKS - create output dictionary by zipping subreddit column from adf and tri_counts into a list of tuples, then passing dict()\n",
        "  # the top 10 most frequent ngrams are obtained by calling sorted() on tri_counts and keeping only the top 10 elements\n",
        "  out_dict = {}\n",
        "  for i, row in adf.iterrows():\n",
        "    out_dict[i] = row['all_words']\n",
        "\n",
        "  return out_dict\n",
        "  "
      ],
      "metadata": {
        "id": "A0NeN7uGftfY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get stopwords as list\n",
        "sw = sw.words('english')\n",
        "# get punctuation as list\n",
        "p = list(string.punctuation)\n",
        "# optional lines for adding the below line to avoid the SettingWithCopyWarning\n",
        "warnings.filterwarnings('ignore')\n",
        "get_tris(df, sw, p)"
      ],
      "metadata": {
        "id": "jLHfnUK_g5vb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26bd6c76-8ac2-48a2-fe92-9d3d2d42d514"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'40kLore': [(('--', '--', '--'), 145),\n",
              "  (('whose', 'bolter', 'anyway'), 8),\n",
              "  (('started', 'us', 'examples'), 8),\n",
              "  (('space', 'marine', 'chapter'), 7),\n",
              "  (('kabal', 'black', 'heart'), 7),\n",
              "  (('lo', 'â€™', 'tos'), 7),\n",
              "  (('die', 'paragon', 'knights'), 6),\n",
              "  (('dark', 'age', 'technology'), 4),\n",
              "  (('let', \"'s\", 'say'), 4),\n",
              "  (('``', 'star', 'claimers'), 4)],\n",
              " 'AMD_Stock': [(('created', 'subreddit', 'reddit'), 10),\n",
              "  (('subreddit', 'reddit', 'posts'), 10),\n",
              "  (('reddit', 'posts', 'r/radeongpus'), 10),\n",
              "  (('open', 'redditors', 'posts'), 10),\n",
              "  (('redditors', 'posts', 'well'), 10),\n",
              "  (('posts', 'well', 'please'), 10),\n",
              "  (('well', 'please', 'consider'), 10),\n",
              "  (('please', 'consider', 'subscribing'), 10),\n",
              "  (('consider', 'subscribing', 'find'), 10),\n",
              "  (('subscribing', 'find', 'posts'), 10)],\n",
              " 'Anki': [(('``', 'conditional', \"''\"), 7),\n",
              "  (('``', 'field2', \"''\"), 5),\n",
              "  (('\\\\^conditional', 'field1', '/conditional'), 5),\n",
              "  (('conditional', \"''\", 'filled'), 4),\n",
              "  (('font-family', 'simplified', 'arabic'), 3),\n",
              "  (('simplified', 'arabic', 'font-size'), 3),\n",
              "  (('export', 'cards', 'selected'), 3),\n",
              "  (('cards', 'selected', 'browser'), 3),\n",
              "  (('anki', \"'s\", 'code'), 3),\n",
              "  (('conditional', 'field1', '/conditional'), 3)],\n",
              " 'ApexOutlands': [(('arc', 'star', 'arc'), 2),\n",
              "  (('star', 'arc', 'star'), 2),\n",
              "  (('cute', 'â€œ', 'mean'), 1),\n",
              "  (('â€œ', 'mean', '..'), 1),\n",
              "  (('mean', '..', 'goo'), 1),\n",
              "  (('..', 'goo', 'goo'), 1),\n",
              "  (('goo', 'goo', 'ga'), 1),\n",
              "  (('goo', 'ga', 'ga'), 1),\n",
              "  (('ga', 'ga', 'â€'), 1),\n",
              "  (('ga', 'â€', 'like'), 1)],\n",
              " 'BanGDream': [(('bang', 'dream', 'girls'), 10),\n",
              "  (('boosts', 'score', 'notes'), 9),\n",
              "  (('full', 'english/romaji', 'lyrics'), 8),\n",
              "  (('low', 'effort', 'edit'), 7),\n",
              "  (('effort', 'edit', 'bang'), 7),\n",
              "  (('edit', 'bang', 'dream'), 7),\n",
              "  (('dream', 'girls', 'initial'), 7),\n",
              "  (('girls', 'initial', 'cars'), 7),\n",
              "  (('initial', 'cars', 'either'), 7),\n",
              "  (('cars', 'either', 'make'), 7)],\n",
              " 'BrandNewSentence': [(('cool', 'cool', 'cool'), 2),\n",
              "  (('unrealistic', 'de-dun-dun-dun', 'grindr'), 1),\n",
              "  (('de-dun-dun-dun', 'grindr', 'straight'), 1),\n",
              "  (('grindr', 'straight', 'guys'), 1),\n",
              "  (('straight', 'guys', \"n't\"), 1),\n",
              "  (('guys', \"n't\", 'start'), 1),\n",
              "  ((\"n't\", 'start', 'fire'), 1),\n",
              "  (('start', 'fire', 'hate'), 1),\n",
              "  (('fire', 'hate', 'apples'), 1),\n",
              "  (('hate', 'apples', 'avoid'), 1)],\n",
              " 'COVID': [(('covid-19', 'products', 'covid-19'), 6),\n",
              "  (('products', 'covid-19', 'test'), 6),\n",
              "  (('covid-19', 'test', 'kits'), 6),\n",
              "  (('test', 'kits', 'covid-19'), 6),\n",
              "  (('kits', 'covid-19', 'face'), 6),\n",
              "  (('covid-19', 'face', 'masks'), 6),\n",
              "  (('face', 'masks', 'protective'), 6),\n",
              "  (('masks', 'protective', 'isolation'), 6),\n",
              "  (('protective', 'isolation', 'coveralls'), 6),\n",
              "  (('isolation', 'coveralls', 'ppe'), 6)],\n",
              " 'COVID19': [(('coronavirus', 'disease', '2019'), 8),\n",
              "  (('get', 'in-depth', 'ysis'), 6),\n",
              "  (('in-depth', 'ysis', 'covid-19'), 6),\n",
              "  (('ysis', 'covid-19', 'impact'), 6),\n",
              "  (('disease', '2019', 'covid-19'), 4),\n",
              "  (('systematic', 'review', 'meta-ysis'), 4),\n",
              "  (('market', 'get', 'in-depth'), 4),\n",
              "  (('nebraska', 'medical', 'center'), 3),\n",
              "  (('among', 'covid-19', 'patients'), 2),\n",
              "  (('severe', 'covid-19', 'patients'), 2)],\n",
              " 'CanadaCoronavirus': [(('--', '-|', '--'), 478),\n",
              "  (('-|', '--', '-|'), 441),\n",
              "  (('pending|', 'pending', '|self-isolating'), 64),\n",
              "  (('|pending|', 'pending', '|pending'), 57),\n",
              "  (('|pending', '|pending', '|pending'), 53),\n",
              "  (('lt', '5', 'may'), 50),\n",
              "  (('pending', '|pending', '|self-isolating'), 46),\n",
              "  (('lt', '5', 'lt'), 45),\n",
              "  (('5', 'lt', '5'), 45),\n",
              "  (('pending', '|pending', '|pending'), 42)],\n",
              " 'China_Flu': [(('covid-19', 'daily', 'discussion'), 36),\n",
              "  (('world', 'health', 'organization'), 14),\n",
              "  (('covid-19', 'weekly', 'discussion'), 12),\n",
              "  (('--', '-|', '--'), 11),\n",
              "  (('growth', 'ðŸ“ˆ', 'increasing'), 10),\n",
              "  (('total', 'confirmed', 'cases'), 10),\n",
              "  (('-|', '--', '-|'), 9),\n",
              "  (('new', 'cases', 'coronavirus'), 8),\n",
              "  (('confirmed', 'cases', 'growth'), 8),\n",
              "  (('death', 'rate', 'growth'), 8)],\n",
              " 'Coronavirus': [(('coronavirus', 'death', 'toll'), 29),\n",
              "  (('new', 'coronavirus', 'cases'), 27),\n",
              "  (('new', 'covid-19', 'cases'), 21),\n",
              "  (('new', 'york', 'city'), 20),\n",
              "  (('tests', 'positive', 'coronavirus'), 16),\n",
              "  (('tested', 'positive', 'covid-19'), 15),\n",
              "  (('new', 'cases', 'coronavirus'), 13),\n",
              "  (('test', 'positive', 'covid-19'), 10),\n",
              "  (('tested', 'positive', 'coronavirus'), 10),\n",
              "  (('tests', 'positive', 'covid-19'), 9)],\n",
              " 'CoronavirusCA': [(('daily', 'check-in/personal', 'thread'), 8),\n",
              "  (('2020', 'concerns', 'vents'), 4),\n",
              "  (('concerns', 'vents', 'questions'), 4),\n",
              "  (('vents', 'questions', 'anecdotes'), 4),\n",
              "  (('questions', 'anecdotes', 'personal'), 4),\n",
              "  (('anecdotes', 'personal', 'preparation'), 4),\n",
              "  (('personal', 'preparation', 'understand'), 4),\n",
              "  (('preparation', 'understand', 'stressful'), 4),\n",
              "  (('understand', 'stressful', 'time'), 4),\n",
              "  (('stressful', 'time', 'community'), 4)],\n",
              " 'CoronavirusCirclejerk': [(('average', 'daily', 'number'), 2),\n",
              "  (('daily', 'number', 'deaths'), 2),\n",
              "  (('number', 'deaths', 'per'), 2),\n",
              "  (('deaths', 'per', 'week'), 2),\n",
              "  (('suggestions', 'better', 'unenforceable'), 1),\n",
              "  (('better', 'unenforceable', 'orders'), 1),\n",
              "  (('unenforceable', 'orders', 'removed'), 1),\n",
              "  (('orders', 'removed', 'â€™'), 1),\n",
              "  (('removed', 'â€™', 'like'), 1),\n",
              "  (('â€™', 'like', 'casey'), 1)],\n",
              " 'CoronavirusDownunder': [(('--', '--', '--'), 48),\n",
              "  (('27', '26', '25'), 26),\n",
              "  (('-|', '-|', '-|'), 23),\n",
              "  (('sept', '27', '26'), 23),\n",
              "  (('26', '25', '24'), 23),\n",
              "  (('lt', '==', '10'), 22),\n",
              "  (('==', '10', 'days'), 22),\n",
              "  (('10', 'days', 'ago'), 22),\n",
              "  (('24', '23', '22'), 19),\n",
              "  (('25', '24', '23'), 18)],\n",
              " 'CoronavirusUK': [(('latest', 'r', 'number'), 18),\n",
              "  (('daily', 'deaths', 'total'), 11),\n",
              "  (('nhs', 'test', 'trace'), 10),\n",
              "  (('nhs',\n",
              "    'england',\n",
              "    '//www.england.nhs.uk/statistics/statistical-work-areas/covid-19-daily-deaths/'),\n",
              "   9),\n",
              "  (('daily', 'deaths', '0'), 9),\n",
              "  (('deaths', '0', 'total'), 9),\n",
              "  (('test', 'trace', 'ask'), 9),\n",
              "  (('trace', 'ask', 'test'), 9),\n",
              "  (('ask', 'test', 'online'), 9),\n",
              "  (('//www.gov.scot/publications/coronavirus-covid-19-daily-data-for-scotland/',\n",
              "    'daily',\n",
              "    'deaths'),\n",
              "   8)],\n",
              " 'CoronavirusUS': [(('growth', 'ðŸ“ˆ', 'increasing'), 9),\n",
              "  (('confirmed', 'cases', 'growth'), 8),\n",
              "  (('total', 'confirmed', 'cases'), 8),\n",
              "  (('death', 'rate', 'growth'), 8),\n",
              "  (('cases', 'growth', 'ðŸ“ˆ'), 6),\n",
              "  (('rate', 'growth', 'mixed'), 5),\n",
              "  (('recovery', 'rate', 'ðŸ“‰'), 5),\n",
              "  (('update', 'coronavirus', 'growth'), 4),\n",
              "  (('data', 'available', 'detail'), 4),\n",
              "  (('available',\n",
              "    'detail',\n",
              "    '//docs.google.com/spreadsheets/d/1w7kqwpsncplcckokvbng8a2nwcn5qo3ydycdcvw38cc/edit'),\n",
              "   4)],\n",
              " 'CovIdiots': [(('refusing', 'wear', 'mask'), 3),\n",
              "  (('people', \"'s\", 'faces'), 2),\n",
              "  (('tests', 'positive', 'covid-19'), 2),\n",
              "  (('staff', 'nursing', 'home'), 1),\n",
              "  (('nursing', 'home', '19'), 1),\n",
              "  (('home', '19', 'died'), 1),\n",
              "  (('19', 'died', 'told'), 1),\n",
              "  (('died', 'told', 'masks'), 1),\n",
              "  (('told', 'masks', 'would'), 1),\n",
              "  (('masks', 'would', 'scare'), 1)],\n",
              " 'CrackheadCraigslist': [(('deal', 'beauty', 'obsession'), 2),\n",
              "  (('much', 'dude', 'looks'), 1),\n",
              "  (('dude', 'looks', 'like'), 1),\n",
              "  (('looks', 'like', 'jane'), 1),\n",
              "  (('like', 'jane', 'knows'), 1),\n",
              "  (('jane', 'knows', 'wants'), 1),\n",
              "  (('knows', 'wants', '....'), 1),\n",
              "  (('wants', '....', 'looks'), 1),\n",
              "  (('....', 'looks', 'authentic'), 1),\n",
              "  (('looks', 'authentic', 'hmmmm'), 1)],\n",
              " 'EngineeringStudents': [(('resume', 'roundtable', \"'re\"), 1),\n",
              "  (('roundtable', \"'re\", 'prepare'), 1),\n",
              "  ((\"'re\", 'prepare', 'career'), 1),\n",
              "  (('prepare', 'career', 'fairs'), 1),\n",
              "  (('career', 'fairs', 'career'), 1),\n",
              "  (('fairs', 'career', 'fairs'), 1),\n",
              "  (('career', 'fairs', 'job'), 1),\n",
              "  (('fairs', 'job', 'applications'), 1),\n",
              "  (('job', 'applications', 'loom'), 1),\n",
              "  (('applications', 'loom', 'near'), 1)],\n",
              " 'FigureSkating': [(('russian', 'championships', '2021'), 3),\n",
              "  (('russian', 'figure', 'skating'), 3),\n",
              "  (('rostelecom', 'cup', '2020'), 2),\n",
              "  (('sp', 'russian', 'championships'), 2),\n",
              "  (('sp', 'russian', 'cup'), 2),\n",
              "  (('4th', 'stage', '2020'), 2),\n",
              "  (('best', 'men', \"'s\"), 2),\n",
              "  (('1.', 'yuzuru', 'hanyu'), 2),\n",
              "  (('yuzuru', 'hanyu', '2.'), 2),\n",
              "  (('us', 'nationals', '2021'), 2)],\n",
              " 'Fusion360': [(('fusion', '360', 'beginner'), 2),\n",
              "  (('360', 'beginner', 'tip'), 2),\n",
              "  (('beginner', 'tip', 'clear'), 2),\n",
              "  (('tip', 'clear', 'selections'), 2),\n",
              "  (('clear', 'selections', 'start'), 2),\n",
              "  (('selections', 'start', 'fusion'), 2),\n",
              "  (('start', 'fusion', '360'), 2),\n",
              "  (('fusion', '360', 'increments'), 1),\n",
              "  (('360', 'increments', 'adjust'), 1),\n",
              "  (('increments', 'adjust', 'drag'), 1)],\n",
              " 'Gameboy': [(('despite', 'several', 'modded'), 1),\n",
              "  (('several', 'modded', 'dmgs'), 1),\n",
              "  (('modded', 'dmgs', 'sometimes'), 1),\n",
              "  (('dmgs', 'sometimes', 'like'), 1),\n",
              "  (('sometimes', 'like', 'going'), 1),\n",
              "  (('like', 'going', 'back'), 1),\n",
              "  (('going', 'back', 'humble'), 1),\n",
              "  (('back', 'humble', 'stock'), 1),\n",
              "  (('humble', 'stock', 'game'), 1),\n",
              "  (('stock', 'game', 'boy'), 1)],\n",
              " 'HolUp': [(('p', 'color', 'blind'), 10),\n",
              "  (('color', 'blind', 'test'), 10),\n",
              "  (('blind', 'test', 'p'), 9),\n",
              "  (('test', 'p', 'color'), 9),\n",
              "  (('holup', 'holup', 'holup'), 7),\n",
              "  (('hol', 'hol', 'hol'), 6),\n",
              "  (('wait', 'minute', '....'), 3),\n",
              "  (('minute', '....', 'wait'), 3),\n",
              "  (('wait', 'minute', '...'), 3),\n",
              "  ((\"'s\", 'wrong', \"'s\"), 2)],\n",
              " 'JoeBiden': [(('--', '--', '--'), 82),\n",
              "  (('joe', 'biden', \"'s\"), 7),\n",
              "  (('..', 'joe', 'biden'), 5),\n",
              "  (('joe', 'biden', 'president'), 5),\n",
              "  (('responded', '``', 'true'), 4),\n",
              "  (('``', 'true', \"''\"), 4),\n",
              "  (('covid', 'covid', 'covid'), 4),\n",
              "  (('state', 'covid', 'deaths'), 4),\n",
              "  (('joe', 'biden', 'kamala'), 3),\n",
              "  (('biden', 'kamala', 'harris'), 3)],\n",
              " 'Konosuba': [(('random', 'thought', 'aqua'), 2),\n",
              "  (('megumin', 'drawn', 'ç›¸å·ã‚Šã‚‡ã†'), 2),\n",
              "  (('took', 'step', 'back'), 2),\n",
              "  (('read', 'comments', 'mom'), 2),\n",
              "  (('megumin', 'vanir', 'wiz'), 1),\n",
              "  (('vanir', 'wiz', 'megumin'), 1),\n",
              "  (('wiz', 'megumin', 'kitsu'), 1),\n",
              "  (('megumin', 'kitsu', 'darkness'), 1),\n",
              "  (('kitsu', 'darkness', 'megumin'), 1),\n",
              "  (('darkness', 'megumin', 'smol'), 1)],\n",
              " 'LivestreamFail': [(('walkthrough', 'gameplay', 'part'), 9),\n",
              "  (('erobb', \"'s\", 'voice'), 3),\n",
              "  ((\"'s\", 'voice', 'cracks'), 3),\n",
              "  (('potty', 'train', 'puppy'), 3),\n",
              "  (('train', 'puppy', 'easily'), 3),\n",
              "  (('puppy', 'easily', 'everything'), 3),\n",
              "  (('easily', 'everything', 'need'), 3),\n",
              "  (('everything', 'need', 'know'), 3),\n",
              "  (('talks', 'first', 'time'), 3),\n",
              "  (('abusing', 'copyright', 'law'), 2)],\n",
              " 'LockdownSkepticism': [(('adjusted', 'odds', 'ratio'), 3),\n",
              "  (('herd', 'immunity', 'â€™'), 2),\n",
              "  (('new', 'york', 'â€™'), 2),\n",
              "  (('vaccine', 'wo', \"n't\"), 2),\n",
              "  (('oxford', 'professor', 'says'), 2),\n",
              "  (('great', 'barrington', 'declaration'), 2),\n",
              "  (('masks', 'social', 'distancing'), 2),\n",
              "  (('people', 'claiming', 'unemployment'), 2),\n",
              "  (('claiming', 'unemployment', 'benefit'), 2),\n",
              "  (('strip', 'club', 'lawsuit'), 2)],\n",
              " 'MensLib': [(('tuesday', 'check', \"'s\"), 1),\n",
              "  (('check', \"'s\", 'everyone'), 1),\n",
              "  ((\"'s\", 'everyone', \"'s\"), 1),\n",
              "  (('everyone', \"'s\", 'mental'), 1),\n",
              "  ((\"'s\", 'mental', 'health'), 1),\n",
              "  (('mental', 'health', 'good'), 1),\n",
              "  (('health', 'good', 'morning'), 1),\n",
              "  (('good', 'morning', 'everyone'), 1),\n",
              "  (('morning', 'everyone', 'welcome'), 1),\n",
              "  (('everyone', 'welcome', 'new'), 1)],\n",
              " 'NintendoSwitch': [(('daily', 'question', 'thread'), 10),\n",
              "  (('animal', 'crossing', 'new'), 9),\n",
              "  (('crossing', 'new', 'horizons'), 8),\n",
              "  (('/r/nintendoswitch', \"'s\", 'daily'), 8),\n",
              "  ((\"'s\", 'daily', 'question'), 8),\n",
              "  ((\"'re\", 'interested', 'becoming'), 8),\n",
              "  (('interested', 'becoming', 'wiki'), 8),\n",
              "  (('becoming', 'wiki', 'contributor'), 8),\n",
              "  (('wiki', 'contributor', 'message'), 8),\n",
              "  (('contributor', 'message', '/u/flapsnapple'), 8)],\n",
              " 'NoLockdownsNoMasks': [(('thailand', 'medical', 'news'), 2),\n",
              "  (('government', 'model', 'suggests'), 1),\n",
              "  (('model', 'suggests', 'u.s.'), 1),\n",
              "  (('suggests', 'u.s.', 'covid-19'), 1),\n",
              "  (('u.s.', 'covid-19', 'cases'), 1),\n",
              "  (('covid-19', 'cases', 'could'), 1),\n",
              "  (('cases', 'could', 'approaching'), 1),\n",
              "  (('could', 'approaching', '100'), 1),\n",
              "  (('approaching', '100', 'million'), 1),\n",
              "  (('100', 'million', 'npr'), 1)],\n",
              " 'NoNewNormal': [(('``', 'covid', \"''\"), 6),\n",
              "  (('``', 'new', 'normal'), 5),\n",
              "  (('new', 'normal', \"''\"), 5),\n",
              "  (('â€', 'â€œ', 'â€™'), 4),\n",
              "  (('word', '``', 'covid'), 4),\n",
              "  (('``', 'great', 'reset'), 4),\n",
              "  (('great', 'reset', \"''\"), 4),\n",
              "  (('people', 'wearing', 'masks'), 3),\n",
              "  (('....', \"''\", \"'m\"), 3),\n",
              "  ((\"''\", \"'m\", 'sorry'), 3)],\n",
              " 'Norse': [(('viking', 'amulet', 'dragon'), 1),\n",
              "  (('amulet', 'dragon', 'fafnir'), 1),\n",
              "  (('dragon', 'fafnir', 'recreated'), 1),\n",
              "  (('fafnir', 'recreated', 'amulets'), 1),\n",
              "  (('recreated', 'amulets', 'usually'), 1),\n",
              "  (('amulets', 'usually', 'referred'), 1),\n",
              "  (('usually', 'referred', 'viking'), 1),\n",
              "  (('referred', 'viking', 'age'), 1),\n",
              "  (('viking', 'age', 'votive'), 1),\n",
              "  (('age', 'votive', 'thor'), 1)],\n",
              " 'PaymoneyWubby': [(('5', '8', \"''\"), 2),\n",
              "  (('funny', 'clips', 'tonight'), 2),\n",
              "  (('discord', 'trying', 'find'), 1),\n",
              "  (('trying', 'find', 'songs'), 1),\n",
              "  (('find', 'songs', 'played'), 1),\n",
              "  (('songs', 'played', 'stream'), 1),\n",
              "  (('played', 'stream', 'today'), 1),\n",
              "  (('stream', 'today', \"n't\"), 1),\n",
              "  (('today', \"n't\", 'popping'), 1),\n",
              "  ((\"n't\", 'popping', 'media'), 1)],\n",
              " 'Pizza': [(('new', 'york', 'style'), 3),\n",
              "  (('months', 'ago', 'sister'), 1),\n",
              "  (('ago', 'sister', 'tried'), 1),\n",
              "  (('sister', 'tried', 'hand'), 1),\n",
              "  (('tried', 'hand', 'deep-dish'), 1),\n",
              "  (('hand', 'deep-dish', 'birthday'), 1),\n",
              "  (('deep-dish', 'birthday', 'known'), 1),\n",
              "  (('birthday', 'known', 'sub'), 1),\n",
              "  (('known', 'sub', 'would'), 1),\n",
              "  (('sub', 'would', 'taken'), 1)],\n",
              " 'PublicFreakout': [(('black', 'lives', 'matter'), 7),\n",
              "  (('Ø¯Ø§Ù†Ù„ÙˆØ¯', 'Ø¢Ù‡Ù†Ú¯', 'Ù…Ø§Ø²Ù†Ø¯Ø±Ø§Ù†ÛŒ'), 4),\n",
              "  (('new', 'york', 'city'), 3),\n",
              "  (('body', 'cam', 'footage'), 3),\n",
              "  (('p', 'color', 'blind'), 3),\n",
              "  (('color', 'blind', 'test'), 3),\n",
              "  (('Ø¯Ø§Ù†Ù„ÙˆØ¯', 'Ø¢Ù‡Ù†Ú¯', 'Ú©Ø±Ø¯ÛŒ'), 3),\n",
              "  (('lives', 'matter', 'protest'), 2),\n",
              "  (('gets', 'racially', 'profiled'), 2),\n",
              "  (('george', 'floyd', 'protests'), 2)],\n",
              " 'SandersForPresident': [(('early', 'voting', 'ends'), 6),\n",
              "  (('bernie', 'sanders', 'president'), 4),\n",
              "  (('bernie', 'sanders', 'â€™'), 4),\n",
              "  (('let', \"'s\", 'get'), 3),\n",
              "  (('bernie', 'sanders', 'nevada'), 3),\n",
              "  (('sen.', 'bernie', 'sanders'), 3),\n",
              "  (('democrats', 'abroad', 'primary'), 3),\n",
              "  (('bernie', 'sanders', 'takes'), 3),\n",
              "  (('bernie', 'sanders', 'bernie'), 3),\n",
              "  ((\"''\", 'bernie', 'sanders'), 3)],\n",
              " 'TheRealJoke': [(('hitting', 'high', 'note'), 1),\n",
              "  (('high', 'note', 'got'), 1),\n",
              "  (('note', 'got', 'ta'), 1),\n",
              "  (('got', 'ta', 'mother'), 1),\n",
              "  (('ta', 'mother', 'real'), 1),\n",
              "  (('mother', 'real', 'jokes'), 1),\n",
              "  (('real', 'jokes', 'guy'), 1),\n",
              "  (('jokes', 'guy', 'took'), 1),\n",
              "  (('guy', 'took', 'comment'), 1),\n",
              "  (('took', 'comment', 'whole'), 1)],\n",
              " 'TheVampireDiaries': [(('surfer', 'wolf', 'guy'), 4),\n",
              "  (('``', 'agree', \"''\"), 3),\n",
              "  (('â€™', 'sure', 'one'), 2),\n",
              "  (('â€™', 'ever', 'seen'), 2),\n",
              "  (('really', 'happened', 'night'), 2),\n",
              "  (('damon', 'killed', 'surfer'), 2),\n",
              "  (('killed', 'surfer', 'wolf'), 2),\n",
              "  (('``', 'accept', 'opinion'), 2),\n",
              "  (('accept', 'opinion', \"''\"), 2),\n",
              "  (('yes', 'klaus', 'caroline'), 1)],\n",
              " 'WTF': [(('sure', 'sneak', 'one'), 1),\n",
              "  (('sneak', 'one', '..'), 1),\n",
              "  (('one', '..', \"'m\"), 1),\n",
              "  (('..', \"'m\", 'march'), 1),\n",
              "  ((\"'m\", 'march', \"'m\"), 1),\n",
              "  (('march', \"'m\", 'going'), 1),\n",
              "  ((\"'m\", 'going', 'need'), 1),\n",
              "  (('going', 'need', 'llama'), 1),\n",
              "  (('need', 'llama', 'firefighters'), 1),\n",
              "  (('llama', 'firefighters', 'surprised'), 1)],\n",
              " 'WindowsMR': [(('hard', 'find', 'replacement'), 1),\n",
              "  (('find', 'replacement', 'controllers'), 1),\n",
              "  (('replacement', 'controllers', 'wmr'), 1),\n",
              "  (('controllers', 'wmr', 'good'), 1),\n",
              "  (('wmr', 'good', 'concept'), 1),\n",
              "  (('good', 'concept', 'replacement'), 1),\n",
              "  (('concept', 'replacement', 'parts'), 1),\n",
              "  (('replacement', 'parts', 'impossible'), 1),\n",
              "  (('parts', 'impossible', 'find'), 1)],\n",
              " 'WitchesVsPatriarchy': [(('``', 'women', 'history'), 3),\n",
              "  (('women', 'history', \"''\"), 3),\n",
              "  (('today', '``', 'women'), 2),\n",
              "  (('given', 'success', '``'), 1),\n",
              "  (('success', '``', 'women'), 1),\n",
              "  (('history', \"''\", 'fictional'), 1),\n",
              "  ((\"''\", 'fictional', 'character'), 1),\n",
              "  (('fictional', 'character', 'entry'), 1),\n",
              "  (('character', 'entry', 'april'), 1),\n",
              "  (('entry', 'april', '1st'), 1)],\n",
              " '[***]og': [(('kodak', 'gold', '200'), 51),\n",
              "  (('kodak', 'portra', '400'), 41),\n",
              "  (('kodak', 'ultramax', '400'), 19),\n",
              "  (('canon', 'ae-1', '50mm'), 15),\n",
              "  (('kodak', 'ektar', '100'), 14),\n",
              "  (('portra', '400', 'mamiya'), 13),\n",
              "  (('fuji', 'superia', '400'), 13),\n",
              "  (('mamiya', 'rb67', 'portra'), 13),\n",
              "  (('rb67', 'portra', '400'), 12),\n",
              "  (('canon', 'eos', '3'), 12)],\n",
              " 'army': [(('fort', 'carson', \"'s\"), 5),\n",
              "  (('fort', 'carson', 'outdoor'), 4),\n",
              "  (('carson', 'outdoor', 'recreation'), 4),\n",
              "  (('mountain', 'post', 'living'), 4),\n",
              "  (('fort', 'carson', 'mountain'), 3),\n",
              "  (('carson', 'mountain', 'post'), 3),\n",
              "  (('post', 'living', 'pcs'), 3),\n",
              "  (('wednesday', 'advice', 'thread'), 2),\n",
              "  (('let', 'us', 'know'), 2),\n",
              "  ((\"'best\", \"'worst\", 'mos'), 2)],\n",
              " 'bleach': [(('--', '--', '--'), 71),\n",
              "  (('court', 'guard', 'squads'), 12),\n",
              "  (('1st', 'level', 'hell'), 7),\n",
              "  (('13', 'court', 'guard'), 7),\n",
              "  (('court', 'guard', 'squad'), 7),\n",
              "  (('traditional', 'soul', 'reaper'), 7),\n",
              "  (('2nd', 'level', 'hell'), 6),\n",
              "  (('3rd', 'level', 'hell'), 6),\n",
              "  (('guard', 'squads', 'hell'), 6),\n",
              "  (('lowest', 'level', 'hell'), 6)],\n",
              " 'brisbane': [(('..', 'cloud', 'report'), 5),\n",
              "  (('prinl', 'place', 'residence'), 4),\n",
              "  (('business', 'activity', 'undertaking'), 4),\n",
              "  (('drink', 'ice', 'cold'), 3),\n",
              "  (('ice', 'cold', 'vb'), 3),\n",
              "  (('cold', 'vb', 'bar'), 3),\n",
              "  (('vb', 'bar', 'fridge'), 3),\n",
              "  (('year', \"'s\", 'ekka'), 2),\n",
              "  ((\"'s\", 'ekka', 'cancelled'), 2),\n",
              "  (('0', 'new', 'covid'), 2)],\n",
              " 'conspiracy': [(('--', '--', '--'), 203),\n",
              "  (('new', 'world', 'order'), 29),\n",
              "  (('black', 'lives', 'matter'), 17),\n",
              "  (('million', 'x200b', 'x200b'), 14),\n",
              "  (('x200b', 'x200b', 'secretary'), 13),\n",
              "  (('military', 'intelligence', 'operation'), 10),\n",
              "  (('royal', 'death', 'racket'), 9),\n",
              "  (('one', 'world', 'government'), 9),\n",
              "  (('thousand', '6', 'months'), 8),\n",
              "  (('10', 'billion', 'trillion'), 8)],\n",
              " 'criminalminds': [(('favourite', 'underrated', 'friendship'), 1),\n",
              "  (('underrated', 'friendship', 'part'), 1),\n",
              "  (('friendship', 'part', '2'), 1),\n",
              "  (('part', '2', 'view'), 1),\n",
              "  (('2', 'view', 'poll'), 1),\n",
              "  (('view', 'poll', '//www.reddit.com/poll/k2rntc'), 1),\n",
              "  (('poll', '//www.reddit.com/poll/k2rntc', 'elle'), 1),\n",
              "  (('//www.reddit.com/poll/k2rntc', 'elle', 'emily'), 1),\n",
              "  (('elle', 'emily', 'felt'), 1),\n",
              "  (('emily', 'felt', 'like'), 1)],\n",
              " 'donaldtrump': [(('donald', 'j.', 'trump'), 27),\n",
              "  (('j.', 'trump', '``'), 22),\n",
              "  (('republican', 'national', 'convention'), 12),\n",
              "  (('joe', 'biden', \"'s\"), 8),\n",
              "  (('national', 'convention', 'night'), 7),\n",
              "  (('2020', 'potus', 'schedule'), 7),\n",
              "  (('team', 'trump', '``'), 6),\n",
              "  (('president', 'trump', \"'s\"), 6),\n",
              "  (('president', 'donald', 'trump'), 6),\n",
              "  (('black', 'lives', 'matter'), 6)],\n",
              " 'gundeals': [(('ship', 'tax', 'az'), 3),\n",
              "  (('coupon', 'code', '``'), 3),\n",
              "  (('nfa', 'yhm', 'turbo'), 2),\n",
              "  (('free', 'shipping', 'acc'), 2),\n",
              "  (('ammo', 'sellier', 'bellot'), 2),\n",
              "  (('rock', 'island', 'armory'), 2),\n",
              "  (('sds', 'imports', '1911'), 2),\n",
              "  (('imports', '1911', 'duty'), 2),\n",
              "  (('pistol', 'grand', 'power'), 2),\n",
              "  (('grand', 'power', 'stribog'), 2)],\n",
              " 'intermittentfasting': [(('dr.', 'jason', 'fung'), 3),\n",
              "  (('intermittent', 'fasting', 'longevity'), 2),\n",
              "  (('removed', 'face', 'gains'), 2),\n",
              "  (('trouble', 'sleeping', 'â€™'), 2),\n",
              "  (('seeing', 'physical', 'changes'), 2),\n",
              "  (('physical', 'changes', 'â€™'), 2),\n",
              "  (('daily', 'intermittent', 'fasting'), 1),\n",
              "  (('intermittent', 'fasting', 'thread'), 1),\n",
              "  (('fasting', 'thread', 'share'), 1),\n",
              "  (('thread', 'share', 'daily'), 1)],\n",
              " 'l4d2': [(('new', 'players', 'like'), 1),\n",
              "  (('players', 'like', 'â€™'), 1),\n",
              "  (('like', 'â€™', 'learnding'), 1),\n",
              "  (('â€™', 'learnding', 'congratulations'), 1),\n",
              "  (('learnding', 'congratulations', 'u/-leblanc-'), 1),\n",
              "  (('congratulations', 'u/-leblanc-', '...'), 1),\n",
              "  (('u/-leblanc-', '...', 'every'), 1),\n",
              "  (('...', 'every', 'match'), 1),\n",
              "  (('every', 'match', 'bad'), 1),\n",
              "  (('match', 'bad', 'guy'), 1)],\n",
              " 'opensource': [(('lemmy', 'open', 'source'), 1),\n",
              "  (('open', 'source', 'decentralized'), 1),\n",
              "  (('source', 'decentralized', 'reddit'), 1),\n",
              "  (('decentralized', 'reddit', 'alternative'), 1),\n",
              "  (('reddit', 'alternative', 'release'), 1),\n",
              "  (('alternative', 'release', 'v0.6.0'), 1),\n",
              "  (('release', 'v0.6.0', 'avatars'), 1),\n",
              "  (('v0.6.0', 'avatars', 'email'), 1),\n",
              "  (('avatars', 'email', 'notifications'), 1),\n",
              "  (('email', 'notifications', 'whole'), 1)],\n",
              " 'playboicarti': [(('\\u200e', '\\u200e', '\\u200e'), 88),\n",
              "  (('whole', 'lotta', 'red'), 35),\n",
              "  (('â€™', 'gon', 'na'), 19),\n",
              "  (('bruh', 'really', 'happening'), 16),\n",
              "  (('gon', 'na', 'get'), 13),\n",
              "  (('really', 'happening', 'bruh'), 12),\n",
              "  (('happening', 'bruh', 'really'), 12),\n",
              "  (('playboi', 'carti', 'removed'), 10),\n",
              "  (('â€™', 'think', 'â€™'), 10),\n",
              "  (('pi', 'â€™', 'erre'), 9)],\n",
              " 'politics': [((\"'ll\", 'right', 'eventually'), 1),\n",
              "  (('right', 'eventually', 'trump'), 1),\n",
              "  (('eventually', 'trump', 'insists'), 1),\n",
              "  (('trump', 'insists', 'coronavirus'), 1),\n",
              "  (('insists', 'coronavirus', 'disappear'), 1),\n",
              "  (('coronavirus', 'disappear', 'citing'), 1),\n",
              "  (('disappear', 'citing', 'evidence'), 1),\n",
              "  (('citing', 'evidence', 'obama'), 1),\n",
              "  (('evidence', 'obama', 'says'), 1),\n",
              "  (('obama', 'says', 'democrat'), 1)],\n",
              " 'razer': [(('razer', 'sea-invitational', '2020'), 7),\n",
              "  (('sea-invitational', '2020', 'dota2'), 5),\n",
              "  (('hdmi', '1', 'dp'), 3),\n",
              "  (('day', '1', 'razer'), 3),\n",
              "  (('lower', 'bracket', 'finals'), 3),\n",
              "  (('899', 'razer', 'raptor'), 2),\n",
              "  (('1', 'hdmi', '1'), 2),\n",
              "  (('1', 'dp', '3.5mm'), 2),\n",
              "  (('dp', '3.5mm', 'audio'), 2),\n",
              "  (('3.5mm', 'audio', 'jack'), 2)],\n",
              " 'rutgers': [(('intro', 'financial', 'accounting'), 3),\n",
              "  (('writing', 'labor', 'studies'), 3),\n",
              "  (('labor', 'studies', 'employment'), 3),\n",
              "  (('studies', 'employment', 'relations'), 3),\n",
              "  (('minor', 'developmental', 'psychology'), 2),\n",
              "  (('intro', 'stats', 'business'), 2),\n",
              "  (('stats', 'business', 'intro'), 2),\n",
              "  (('business', 'intro', 'financial'), 2),\n",
              "  (('refund', 'term', 'bill'), 2),\n",
              "  (('know', 'easy', 'upper'), 2)],\n",
              " 'sony': [(('sony', 'ifa', '2020'), 3),\n",
              "  (('looking', 'forward', 'sony'), 2),\n",
              "  (('forward', 'sony', 'ifa'), 2),\n",
              "  (('ifa', '2020', 'press'), 1),\n",
              "  (('2020', 'press', 'conference'), 1),\n",
              "  (('press', 'conference', 'going'), 1),\n",
              "  (('conference', 'going', 'ahead'), 1),\n",
              "  (('going', 'ahead', 'â€™'), 1),\n",
              "  (('ahead', 'â€™', 'sony'), 1),\n",
              "  (('â€™', 'sony', 'modify'), 1)],\n",
              " 'sportsbook': [(('//www.reddit.com/r/sportsbook/search', 'q=title', '3a'),\n",
              "   28),\n",
              "  (('monthly', '//www.reddit.com/r/sportsbook/search', 'q=title'), 21),\n",
              "  (('removed', 'sports', 'predictions'), 9),\n",
              "  (('sportsbook', 'list', '//www.reddit.com/r/sportsbook/wiki/sportsbooks'),\n",
              "   7),\n",
              "  (('list', '//www.reddit.com/r/sportsbook/wiki/sportsbooks', '/r/sportsbook'),\n",
              "   7),\n",
              "  (('//www.reddit.com/r/sportsbook/wiki/sportsbooks', '/r/sportsbook', 'chat'),\n",
              "   7),\n",
              "  (('/r/sportsbook', 'chat', '//discordapp.com/invite/0z5fkengsblokq4s'), 7),\n",
              "  (('chat', '//discordapp.com/invite/0z5fkengsblokq4s', 'general'), 7),\n",
              "  (('//discordapp.com/invite/0z5fkengsblokq4s',\n",
              "    'general',\n",
              "    'discussion/questions'),\n",
              "   7),\n",
              "  (('general', 'discussion/questions', 'biweekly'), 7)],\n",
              " 'touhou': [(('--', '--', '--'), 36),\n",
              "  (('touhou', 'music', 'thread'), 18),\n",
              "  (('doujin', 'mix', 'touhou'), 9),\n",
              "  (('index',\n",
              "    'thread',\n",
              "    '//old.reddit.com/r/touhou/comments/einuiz/touhou_music_thread_index/'),\n",
              "   7),\n",
              "  (('gensokyo', 'would', 'rather'), 6),\n",
              "  (('index',\n",
              "    'thread',\n",
              "    '//www.reddit.com/r/touhou/comments/gatvxg/touhou_music_thread_index_windows/'),\n",
              "   6),\n",
              "  (('daily', 'gensokyo', 'would'), 5),\n",
              "  (('ahead', 'lt', 'thought'), 5),\n",
              "  (('lt', 'thought', 'play'), 5),\n",
              "  (('thread',\n",
              "    '//old.reddit.com/r/touhou/comments/einuiz/touhou_music_thread_index/',\n",
              "    'touhou'),\n",
              "   5)],\n",
              " 'virginvschad': [(('chadcano', 'season', '4'), 3),\n",
              "  (('season', '4', 'episode'), 2),\n",
              "  (('names', 'thad', 'tracy'), 2),\n",
              "  (('would', 'like', 'announce'), 2),\n",
              "  (('official', 'vvc', 'height'), 1),\n",
              "  (('vvc', 'height', 'incel'), 1),\n",
              "  (('height', 'incel', 'bruce'), 1),\n",
              "  (('incel', 'bruce', '..'), 1),\n",
              "  (('bruce', '..', 'nonexistentad'), 1),\n",
              "  (('..', 'nonexistentad', 'behold'), 1)],\n",
              " 'wicked_edge': [(('full', 'show', 'online'), 6),\n",
              "  (('show', 'online', 'free'), 6),\n",
              "  (('cyril', 'r.', 'salter'), 5),\n",
              "  (('bulgari', 'man', 'black'), 3),\n",
              "  (('r.', 'salter', 'french'), 3),\n",
              "  (('salter', 'french', 'vetiver'), 3),\n",
              "  (('crs', 'french', 'vetiver'), 3),\n",
              "  (('english', 'subbed', 'watch'), 3),\n",
              "  (('sotd', 'sotn', 'sotd'), 3),\n",
              "  (('zingari', 'man', 'blacksmith'), 2)],\n",
              " 'worldbuilding': [(('little', 'lore', 'game'), 10),\n",
              "  (('lore', 'game', 'coming'), 10),\n",
              "  (('game', 'coming', 'small'), 10),\n",
              "  (('coming', 'small', 'things'), 10),\n",
              "  (('small', 'things', 'worlds'), 10),\n",
              "  (('things', 'worlds', 'challenges'), 10),\n",
              "  (('worlds', 'challenges', 'posted'), 10),\n",
              "  (('trying', 'get', 'something'), 10),\n",
              "  (('get', 'something', 'world'), 10),\n",
              "  (('something', 'world', 'nothing'), 10)],\n",
              " 'xqcow': [(('lego', 'harry', 'potter'), 10),\n",
              "  (('harry', 'potter', 'babyrage'), 10),\n",
              "  (('potter', 'babyrage', 'lego'), 9),\n",
              "  (('babyrage', 'lego', 'harry'), 9),\n",
              "  (('posting', 'widepeepohappy', 'xqc'), 8),\n",
              "  (('widepeepohappy', 'xqc', 'stares'), 6),\n",
              "  (('xqc', 'stares', 'camera'), 6),\n",
              "  (('stares', 'camera', 'says'), 6),\n",
              "  (('camera', 'says', 'widepeepohappy'), 6),\n",
              "  (('says', 'widepeepohappy', 'day'), 3)]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8E010UbQyML"
      },
      "source": [
        "## P1.2 - Answering questions with pandas (15 marks)\n",
        "\n",
        "In this question, your task is to use pandas to answer questions about the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZmG2VIYQ93I"
      },
      "source": [
        "### P1.2.1 - Authors that post highly commented posts (3 marks)\n",
        "\n",
        "Find the top 1000 most commented posts. Then, obtain the names of the authors that have at least 3 posts among these posts.\n",
        "\n",
        "**What to implement:** Implement a function `find_popular_authors(df)` that takes as input the original dataframe and returns a list strings, where each string is the name of authors that satisfy the above criteria."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Defintion: Implement a function find_popular_authors(df) that takes as input\n",
        "              the original dataframe and returns a list strings, where each string \n",
        "              is the name of authors that have at least 3 posts in the top 1000 most commented posts\n",
        "\n",
        "   Input:     we take the dataframe as the input\n",
        "\n",
        "   Output:    Returns a list of the popular authors'''\n",
        "\n",
        "def find_popular_authors(df):\n",
        "  sorted_df = df.sort_values(by=['num_comments'], ascending=False).head(1000)\n",
        "  out_list = []\n",
        "  for i, row in sorted_df.iterrows():\n",
        "    if row['subr_numb_posts'] >= 3:\n",
        "      out_list.append(row['author'])  \n",
        "  return list(set(out_list))"
      ],
      "metadata": {
        "id": "URKIW6oMvrYu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_popular_authors(df)"
      ],
      "metadata": {
        "id": "tmsJyZ1_xpGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cd9e6da-a0ba-4405-ee1b-7a2dfc035bfe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Roka-chan',\n",
              " 'DominusDK',\n",
              " 'ArsenalWillBeBack',\n",
              " 'Not4Reel',\n",
              " 'theduck1893',\n",
              " 'BebeFanMasterJ',\n",
              " 'invisiblefaction2',\n",
              " 'MakeItRainSheckels',\n",
              " 'Rude-Mushroom',\n",
              " 'q[***]erras',\n",
              " 'Allstarhit',\n",
              " 'Altruistic_Astronaut',\n",
              " 'Pretty_iin_Pink',\n",
              " '5nordehacedod',\n",
              " 'politicalthrow99',\n",
              " 'praaany',\n",
              " 'FredoSosa',\n",
              " 'acmed',\n",
              " 'mostaksaif',\n",
              " 'CeballaLight7',\n",
              " 'royroy',\n",
              " '-eDgAR-',\n",
              " 'WorkTomorrow',\n",
              " 'quietpilgrim',\n",
              " '61539',\n",
              " 'esberat',\n",
              " 'anon7935678',\n",
              " 'ryanaire5',\n",
              " 'le_br1t',\n",
              " 'VeganSamura1',\n",
              " 'johnslegers',\n",
              " 'progress18',\n",
              " 'n0x0ne',\n",
              " 'gameskull11',\n",
              " 'dodgyb',\n",
              " 'ErinInTheMorning',\n",
              " 'samzz41',\n",
              " 'bgny',\n",
              " 'weed[***]',\n",
              " 'Cicada200',\n",
              " 'Madd-Nigrulo',\n",
              " 'Dark_Diggler',\n",
              " 'Majnum',\n",
              " 'DaRandomGitty2',\n",
              " 'HippolasCage',\n",
              " 'itsreallyreallytrue',\n",
              " 'Gambit08',\n",
              " 'eff50',\n",
              " 'JordanCarter77',\n",
              " 'burtzev',\n",
              " 'Kayozuki',\n",
              " 'strngerdngermaus',\n",
              " 'TobySomething',\n",
              " 'mkbt',\n",
              " 'kevinowdziej',\n",
              " 'dunphish64',\n",
              " 'wesoly17',\n",
              " 'AristonD',\n",
              " 'xanaxcervix',\n",
              " 'Humble_Award_4873',\n",
              " 'chrisdh79',\n",
              " 'factfind',\n",
              " 'karmagheden',\n",
              " 'schuey_08',\n",
              " 'Fr1sk3r',\n",
              " 'KatieAllTheTime',\n",
              " 'Typoqueen00',\n",
              " 'spock23',\n",
              " 'jetvacjesse',\n",
              " 'joyousjoyness',\n",
              " 'LuanderSchwarze',\n",
              " 'AdamCannon',\n",
              " 'n0b0dyn00ne001',\n",
              " 'werdmouf',\n",
              " 'mliash',\n",
              " '_gschaftlhuaba',\n",
              " 'FromMN2AZ2017',\n",
              " 'No-NPC-HERE',\n",
              " 'bored_in_NE',\n",
              " 'ZOEXofficial',\n",
              " 'ghooosst',\n",
              " 'KimJongMakeEmSayUn',\n",
              " 'diveonfire',\n",
              " 'ravedog',\n",
              " 'BlueIce5',\n",
              " 'tttrrruuu',\n",
              " 'illuminata8',\n",
              " 'justcasty',\n",
              " 'Riomegon',\n",
              " 'retalaznstyle',\n",
              " 'MaiSenpaii2',\n",
              " 'chovy',\n",
              " 'Deshes011',\n",
              " 'Playaguy',\n",
              " 'LisaMck041',\n",
              " 'CapitalCourse',\n",
              " 'kirby__000',\n",
              " 'boomerpro',\n",
              " 'conam25',\n",
              " 'DroppingPing',\n",
              " 'Joe_Tazuna',\n",
              " 'Romano16',\n",
              " 'Jah_Wobble',\n",
              " 'TAKEitTOrCIRCLEJERK',\n",
              " 'ObnoxiousOldBastard',\n",
              " 'Iarguewith[***]s',\n",
              " 'patiencetruth',\n",
              " 's4nskrit',\n",
              " 'bobbyw24',\n",
              " 'TaurusII',\n",
              " 'epiphanyx99',\n",
              " 'txiao007',\n",
              " 'alwayswashere',\n",
              " 'asad1ali2',\n",
              " 'AnakinWayneII',\n",
              " 'TurtleFacts72',\n",
              " 'hunt4redglocktober',\n",
              " 'KSCGA',\n",
              " 'JLBesq1981',\n",
              " 'kadirba68',\n",
              " 'SUPERGUESSOUS',\n",
              " 'tizthewizard',\n",
              " 'BadlyShapedKnees',\n",
              " 'exmoor456',\n",
              " 'SuperJohnny25',\n",
              " 'ArtistDecor',\n",
              " 'signed7',\n",
              " 'manmeet10',\n",
              " 'foodforthinks',\n",
              " 'Salramm01',\n",
              " 'mendelevium34',\n",
              " 'LateData',\n",
              " 'Sleepy_Bob_Ross',\n",
              " 'notpreposterous',\n",
              " 'GrandpaChainz',\n",
              " 'Normiesreeee69',\n",
              " 'Avyxyva',\n",
              " 'Iwannadrinkthebleach',\n",
              " 'kapetankuka',\n",
              " 'Hyorennn',\n",
              " 'darkoms666',\n",
              " 'Caiyul',\n",
              " 'clemaneuverers',\n",
              " 'ProudAmerican_MO',\n",
              " 'shookqueen',\n",
              " 'tefunka',\n",
              " 'OnHolidayHere',\n",
              " 'covidditing',\n",
              " 'Mcnst',\n",
              " 'litbacod4',\n",
              " 'ufgman',\n",
              " 'relevantlife',\n",
              " '11_gop',\n",
              " 'allofusahab',\n",
              " 'Nihilist911',\n",
              " 'dsbwayne',\n",
              " 'Thedepressionoftrees',\n",
              " 'Gamepwn',\n",
              " 'casualbadideas',\n",
              " 'AddictedReddit',\n",
              " 'rebooted_life_42',\n",
              " 'atomolayanatomay',\n",
              " 'Infjuk',\n",
              " 'DeWallenVanWimKok',\n",
              " 'ofcabbagesandkings14',\n",
              " 'AlitaBattlePringleTM',\n",
              " 'I_am_a_[***]_to_ants',\n",
              " 'TheJeck',\n",
              " 'ostrichesarenotreal',\n",
              " 'okabe_rintau',\n",
              " 'bobby_triple',\n",
              " 'Lshim',\n",
              " 'DWillGlobal',\n",
              " 'Ch33rn0',\n",
              " 'Kreygasm2233',\n",
              " 'globalhumanism',\n",
              " 'soldio101',\n",
              " 'Gboard2',\n",
              " 'mynameisalex1',\n",
              " 'platxerath',\n",
              " 'vanish619',\n",
              " 'PMSlimeKing',\n",
              " 'henryhuff',\n",
              " 'SufficientHistorian6',\n",
              " 'jerish29',\n",
              " 'Raaapid',\n",
              " 'SayLittleDoMuch',\n",
              " 'wTone_',\n",
              " 'DogMeatTalk',\n",
              " 'MyPenisIsALesbian',\n",
              " 'censor_this_commie',\n",
              " 'Shi[***]ihost',\n",
              " 'pathogenalpha',\n",
              " 'scydude',\n",
              " 'AllisonGator',\n",
              " 'ratioetlogicae',\n",
              " 'jsinkwitz',\n",
              " 'ctrlaltdelmarva',\n",
              " 'madman320',\n",
              " 'IanMazgelis',\n",
              " 'morememesplease',\n",
              " 'redmambo_no6',\n",
              " 'PropagandaDebunker',\n",
              " 'Robin_7883',\n",
              " 'bemani4u',\n",
              " 'GuthixIsBalance',\n",
              " 'Underachlever',\n",
              " 'atomic[***]e',\n",
              " 'try-bi-sum-[***]',\n",
              " 'Jkid',\n",
              " 'Mimi108',\n",
              " 'ghostmeharder',\n",
              " 'Ragnarokcometh',\n",
              " 'kaze_ni_naru',\n",
              " '-ZeuS--',\n",
              " 'CLO_Junkie',\n",
              " 'Willis8604',\n",
              " 'Riptide559',\n",
              " 'Balls_of_Adamanthium',\n",
              " 'puppuli',\n",
              " 'callmebaiken',\n",
              " 'nnnarbz',\n",
              " 'very_zesty_zach',\n",
              " 'dukey',\n",
              " 'mostrandomguy',\n",
              " 'Facts-Over-Opinion',\n",
              " 'T1D2',\n",
              " 'SlobBarker',\n",
              " 'Chobits_',\n",
              " 'FearLess_Alpha',\n",
              " 'daysgoneby27',\n",
              " 'Suxdrip',\n",
              " 'mission_improbables',\n",
              " 'Giraffecaster',\n",
              " 'justbeho',\n",
              " 'notinferno',\n",
              " 'society0',\n",
              " '0ldManFrank',\n",
              " 'StardustRedditor',\n",
              " 'agoodsolidthrowaway',\n",
              " 'icedpickles',\n",
              " 'Zhana-Aul',\n",
              " '_NoSoup4You',\n",
              " 'PlenitudeOpulence',\n",
              " 'Morihando',\n",
              " 'Jumido730',\n",
              " 'TampaBayTimes',\n",
              " 'EyeWikeWocketz',\n",
              " 'Saint_eX',\n",
              " 'Scoopide[***]',\n",
              " 'jcepiano',\n",
              " 'caststonegl[***]home',\n",
              " 'Fuzier',\n",
              " 'cyrusasu',\n",
              " 'DoseofTrainwreckstv',\n",
              " 'speckz',\n",
              " 'Turbostrider27',\n",
              " 'DesignerAttitude98',\n",
              " 'Antiliani',\n",
              " 'MisterT12',\n",
              " 'edgar-reed',\n",
              " 'harushiga',\n",
              " 'HugeDetective0',\n",
              " 't0pt0p',\n",
              " 'bionista',\n",
              " 'Orisaaaaa',\n",
              " 'Al-Andalusia',\n",
              " 'stealthyfrog',\n",
              " 'professorMob',\n",
              " 'lanqian',\n",
              " 'Good-N-2020',\n",
              " 'Glad-Software',\n",
              " 'fallenkeith2018',\n",
              " 'zsreport',\n",
              " 'hilltopye',\n",
              " 'koolman631',\n",
              " 'GlobalCitizen12345',\n",
              " 'SSBM_Cob',\n",
              " 'Aerobics111',\n",
              " 'Canuknucklehead',\n",
              " 'Fickkissen',\n",
              " 'orgiasticfuture',\n",
              " 'SamCFC___',\n",
              " 'Kiddy7180',\n",
              " 'Dannerz',\n",
              " 'whk1992',\n",
              " 'bmac3434',\n",
              " 'mythrowawaybabies',\n",
              " 'invertedparadX',\n",
              " 'Juicyjackson',\n",
              " 'AutoModerator',\n",
              " 'stuuked',\n",
              " 'IronWolve',\n",
              " 'Dajakesta0624',\n",
              " 'swingadmin',\n",
              " 'Ramy_',\n",
              " 'Magro18',\n",
              " 'pantagathus01',\n",
              " 'truthwillout777',\n",
              " 'Gari_305',\n",
              " 'jblackmiser',\n",
              " 'somnifacientsawyer',\n",
              " 'ArminTv',\n",
              " 'meektakeL',\n",
              " 'mobo392',\n",
              " 'ilove[***]77',\n",
              " 'L-Z-1',\n",
              " 'lukeu42',\n",
              " 'i[***]y[***]n',\n",
              " 'iSlingShlong',\n",
              " 'Ninten-Doh',\n",
              " 'CantStopPoppin',\n",
              " 'NewAltWhoThis',\n",
              " 'stargem5',\n",
              " 'Waldonville',\n",
              " 'lgigi69',\n",
              " 'kanye5150',\n",
              " 'TrumpSharted',\n",
              " 'seamslegit',\n",
              " 'ElectronicFudge5',\n",
              " 'TheAtheistArab87',\n",
              " 'jo_jo_nyeb',\n",
              " 'HighRoller390',\n",
              " 'ReginaldJohnston',\n",
              " 'into_the_[***]e',\n",
              " 'twistedlogicx',\n",
              " 'Sleegan',\n",
              " 'GlobalConcentrate7',\n",
              " 'nightcloudsky',\n",
              " 'jollygreenscott91',\n",
              " 'wolfsog23',\n",
              " 'TSCSN',\n",
              " 'InternetCaesar',\n",
              " 'Saibasaurus',\n",
              " 'meister2a',\n",
              " 'lickmy[***]609',\n",
              " 'BlindingTwilight',\n",
              " 'Havvocck2',\n",
              " 'lexytheblasian',\n",
              " 'Evoqu_',\n",
              " 'arbitopi',\n",
              " 'kirose',\n",
              " 'oliver_21',\n",
              " 'allicat83',\n",
              " 'sour_creme',\n",
              " 'rspix000',\n",
              " 'Steven1958',\n",
              " 'miketheman0506',\n",
              " 'thriftyqueen',\n",
              " 'Singularitytracker',\n",
              " 'DeadEndFred',\n",
              " 'Facerealityalready',\n",
              " 'MysteriiousComposer',\n",
              " 'TheIlluminatiEatPoo',\n",
              " 'BlanketMage',\n",
              " 'hildebrand_rarity',\n",
              " 'BearsNecessity',\n",
              " 'UpNDownCan',\n",
              " 'DoremusJessup',\n",
              " 'theinfinitelight',\n",
              " 'shaman1011',\n",
              " 'TLDR_Swinton',\n",
              " 'dustin_ginsberg',\n",
              " '[***]eandethercreature',\n",
              " 'Grtrshop',\n",
              " 'gabrysanto',\n",
              " 'TheGamerDanYT',\n",
              " 'lilmcfuggin',\n",
              " 'marinatingpandemic',\n",
              " 'Cloud9Shopper',\n",
              " 'KuduIO',\n",
              " 'Gonzo_B',\n",
              " 'SupCJ',\n",
              " 'doththedoth',\n",
              " 'Defie-LOH-Gic',\n",
              " 'johnruby',\n",
              " 'Arkam_slayer66',\n",
              " 'opiumcx',\n",
              " 'joshlreddit',\n",
              " 'r[***]og',\n",
              " '[***]reader',\n",
              " 'enterprisevalue',\n",
              " 'buoninachos',\n",
              " 'Jokengonzo',\n",
              " 'apocalypticalley',\n",
              " 'skuzgang',\n",
              " 'Frocharocha',\n",
              " 'ImNotHereStopAsking',\n",
              " 'BanDerUh',\n",
              " 'opi_oid',\n",
              " 'cosmicprank',\n",
              " 'akarim5847',\n",
              " 'Przemek0980',\n",
              " 'mouthofreason',\n",
              " 'kent_k',\n",
              " 'dadboddadjokes',\n",
              " 'suitabledz',\n",
              " 'Chap82',\n",
              " 'Mahomeboy_',\n",
              " 'Xeelee1123',\n",
              " 'maize-n-blue98',\n",
              " 'CriminalAir',\n",
              " 'arctic-gold-digger',\n",
              " 'genericwan',\n",
              " 'Molire',\n",
              " 'RabidNemo',\n",
              " 'NYLaw',\n",
              " 'throwaway888253',\n",
              " 'Jellyrollrider',\n",
              " 'Gdileavemealone',\n",
              " 'sbpotdbot',\n",
              " 'XDitto',\n",
              " 'rickmartingt',\n",
              " 'HeinieKa[***]ler',\n",
              " 'Vasallo7G',\n",
              " 'Fuckyousantorum',\n",
              " 'FunPeach0',\n",
              " 'jakerepp15',\n",
              " 'lilstinky[***]',\n",
              " 'samarai4444',\n",
              " 'eastbayted',\n",
              " 'abdouh15',\n",
              " 'Hafomeng',\n",
              " 'invertedparado[***]',\n",
              " 'BumblesAZ',\n",
              " 'tacolben',\n",
              " 'mvpeast',\n",
              " 'missuncleben',\n",
              " 'jigsawmap',\n",
              " 'toneii',\n",
              " 'linaching',\n",
              " 'helpfuldare',\n",
              " 'freq-ee',\n",
              " 'axolotl_peyotl',\n",
              " 'NotsoPG',\n",
              " 'Maximus_Ballsackus',\n",
              " 'Driscoll17',\n",
              " 'grantalx',\n",
              " 'KenshiroTheKid',\n",
              " 'OldFashionedJizz',\n",
              " 'MrRoxx',\n",
              " 'OgranismAtWork',\n",
              " 'Coffeboii4real',\n",
              " 'AbominableAnon',\n",
              " 'TitoHernandez',\n",
              " 'jaymar01',\n",
              " 'skenoj',\n",
              " 'Gayfetus',\n",
              " 'SverreTM',\n",
              " 'Eli-Azrael',\n",
              " 'BrunoofBrazil',\n",
              " 'JinaJoe',\n",
              " 'mepper',\n",
              " 'memezzer',\n",
              " 'Ravenbtw',\n",
              " 'farhan9835',\n",
              " 'ohnoh18',\n",
              " 'Smileitsolga',\n",
              " 'Skullzrulerz',\n",
              " 'Vegardand',\n",
              " 'ChiefAzrael',\n",
              " 'ReactionAndy',\n",
              " 'HeftyArt4',\n",
              " 'SAMUEL_L_JACKASS',\n",
              " 'blacked_lover',\n",
              " 'Dystopiannie',\n",
              " '2020c[***]er[***]',\n",
              " 'MisterSpiny',\n",
              " 'DoodlebobIsMy[***]',\n",
              " 'NewShooterComingOut',\n",
              " 'Leg_holes',\n",
              " 'seanspeaks77',\n",
              " 'chakalakasp',\n",
              " 'KitKatHasClaws',\n",
              " 'faab64',\n",
              " 'Kwindecent_exposure',\n",
              " 'The1stCitizenOfTheIn',\n",
              " 'jaimelancaster',\n",
              " 'furretguy',\n",
              " 'ThegrayD',\n",
              " 'magnolyuh',\n",
              " 'DaFunkJunkie',\n",
              " 'AintEverLucky',\n",
              " 'sailorjupiter28titan',\n",
              " 'elt0p0',\n",
              " 'SemperPereunt',\n",
              " 'kevinmrr',\n",
              " 'scamaltert',\n",
              " 'Hellollie',\n",
              " 'kimcheefarts',\n",
              " 'hash0t0',\n",
              " 'BaldLurker',\n",
              " 'nycsellit4me',\n",
              " 'jafreese98',\n",
              " 'My_Memes_Will_Cure_U',\n",
              " 'FringeCenterPodcast',\n",
              " 'noahsurvived',\n",
              " 'pandabeardnm',\n",
              " 'cottoncandy240',\n",
              " 'habichuelacondulce',\n",
              " 'shylock92008',\n",
              " 'ninjyte',\n",
              " 'Razerer92',\n",
              " 'ggstardust69',\n",
              " 'memelord_harsh',\n",
              " 'Cannibaloxfords10',\n",
              " 'masked-n-anonymous',\n",
              " 'SonictheManhog',\n",
              " 'PedaleFrancisca',\n",
              " 'Gringo_Please',\n",
              " 'koavf',\n",
              " 'MrEmerald2006',\n",
              " 'bear-rah',\n",
              " 'PurestVideos',\n",
              " 'theitguyforever',\n",
              " 'amichail',\n",
              " 'MacPepper',\n",
              " 'inspiration_capsule',\n",
              " 'kogeliz',\n",
              " 'Stoaticor',\n",
              " 'Defie-LOH-Giq',\n",
              " 'cdillon42',\n",
              " 'wwwiphala',\n",
              " 'north0east',\n",
              " 'Wagamaga',\n",
              " 'Venus230',\n",
              " 'geoxol',\n",
              " 'IsaacM42',\n",
              " 'Sabremesh',\n",
              " 'lightiggy',\n",
              " 'Kinmuan',\n",
              " 'cfbovernfl',\n",
              " 'dat1guy420',\n",
              " 'One_Curious_Jay',\n",
              " 'return2ozma',\n",
              " 'Mister_That_Guy',\n",
              " 'walkinman19',\n",
              " 'Pessimist2020',\n",
              " 'stem12345679',\n",
              " 'imagepoem',\n",
              " 'xrangegod1',\n",
              " 'Livelikethelotus',\n",
              " 'Zenbach',\n",
              " 'TheFearlessWarrior',\n",
              " 'Lost_Distribution546',\n",
              " 'vizard673',\n",
              " 'avivi_',\n",
              " 'None',\n",
              " 'Cross_Ange',\n",
              " 'discocrisco',\n",
              " 'CaptainTomato21',\n",
              " 'SleepNowMyThrowaway',\n",
              " 'pikcoolski',\n",
              " 'Looddak',\n",
              " 'pranksterdankster',\n",
              " 'iDannyEL',\n",
              " 'xItacolomix']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrl0kq09dxrp"
      },
      "source": [
        "### P1.2.2 - Distribution of posts per weekday (5 marks)\n",
        "\n",
        "Find the percentage of posts that were posted in each weekday (Monday, Tuesday, etc.). You can use an external calendar or you can use any functionality for dealing with dates available in pandas. \n",
        "\n",
        "**What to implement:** A function `get_weekday_post_distribution(df)` that takes as input the original dataframe and returns a dictionary of the form (the values are made up):\n",
        "\n",
        "```\n",
        "{'Monday': '14%',\n",
        "'Tuesday': '23%', \n",
        "...\n",
        "}\n",
        "```\n",
        "\n",
        "Note that you must only return two decimals, and you must include the percentage sign in the output dictionary. \n",
        "\n",
        "Note that in dictionaries order is not preserved, so the order in which it gets printed will not matter. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Defintion: We define a function get_weekday_post_distribution(df)\n",
        "              that takes as input the original dataframe and returns a dictionary\n",
        "              to find the percentage of the posts that are posted each weekday\n",
        "\n",
        "   Inputs:    We take the original dataframe as the input\n",
        "\n",
        "   Outputs:   Returns a dictionary with the percentage up to two decimal places'''\n",
        "\n",
        "def get_weekday_post_distribution(df):\n",
        "  date_to_weekdays = []\n",
        "  for i, row in df.iterrows():\n",
        "    date_to_weekdays.append(datetime.strptime(row['posted_at'], '%Y-%m-%d %H:%M:%S').strftime('%A'))\n",
        "\n",
        "  \n",
        "  total_number_of_rows = df.shape[0]\n",
        "  return {\n",
        "      \"Monday\":f\"{(date_to_weekdays.count('Monday')/total_number_of_rows)*100:.2f}%\",\n",
        "      \"Tuesday\":f\"{(date_to_weekdays.count('Tuesday')/total_number_of_rows)*100:.2f}%\",\n",
        "      \"Wednesday\":f\"{(date_to_weekdays.count('Wednesday')/total_number_of_rows)*100:.2f}%\",\n",
        "      \"Thursday\":f\"{(date_to_weekdays.count('Thursday')/total_number_of_rows)*100:.2f}%\",\n",
        "      \"Friday\":f\"{(date_to_weekdays.count('Friday')/total_number_of_rows)*100:.2f}%\",\n",
        "      \"Saturday\":f\"{(date_to_weekdays.count('Saturday')/total_number_of_rows)*100:.2f}%\",\n",
        "      \"Sunday\":f\"{(date_to_weekdays.count('Sunday')/total_number_of_rows)*100:.2f}%\"\n",
        "  }"
      ],
      "metadata": {
        "id": "Aj_2ss9jy9WC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_weekday_post_distribution(df)"
      ],
      "metadata": {
        "id": "5FUpOzgN1t3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5fd3dbe-dde1-4cac-f308-9ae84b22c921"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Monday': '14.31%',\n",
              " 'Tuesday': '14.54%',\n",
              " 'Wednesday': '14.89%',\n",
              " 'Thursday': '14.75%',\n",
              " 'Friday': '14.79%',\n",
              " 'Saturday': '13.76%',\n",
              " 'Sunday': '12.96%'}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVj1WikSUPjO"
      },
      "source": [
        "### P1.2.3 - The 100 most passionate redditors (7 marks)\n",
        "\n",
        "We would like to know which are the 100 redditors (`author` column) that are most passionate. We will measure this by checking, for each redditor, the ratio at which they use adjectives. This ratio will be computed by dividing number of adjectives by the total number of words each redditor used. The analysis will only consider redditors that have written at least 1000 words.\n",
        "\n",
        "**What to implement:** A function called `get_passionate_redditors(df)` that takes as input the original dataframe and returns a list of the top 100 redditors (authors) by the ratio at which they use adjectives considering both the `title` and `selftext` columns. The returned list should be a list of tuples, where each inner tuple has two elements: the redditor (author) name, and the ratio of adjectives they used. The returned list should be sorted by adjective ratio in descending order (highest first). Only redditors that wrote more than 1000 words should be considered. You should use `nltk`'s `word_tokenize` and `pos_tag` functions to tokenize and find adjectives. You do not need to do any preprocessing like stopword removal, lemmatization or stemming."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Definition: We define a function called get_passionate_redditors(df) \n",
        "               that takes as input the original dataframe and returns a list of the top 100 \n",
        "               redditors (authors) by the ratio at which they use adjectives considering \n",
        "               both the title and selftext columns.\n",
        "\n",
        "   Inputs:     We take the dataframe as the input\n",
        "\n",
        "   Outputs:    We return a list of tuples with the author names and the ratio of the adjectives they used.''' \n",
        "\n",
        "def ratio_cal(row):\n",
        "  wordsList = nltk.word_tokenize(row['full_text'])\n",
        "  tagged = nltk.pos_tag(wordsList)\n",
        "  adj_count = 0\n",
        "  for t in tagged:\n",
        "    if t[1] in ['JJ', 'JJR','JJS']:\n",
        "      adj_count += 1\n",
        "  return adj_count/row['full_text_len']\n",
        "\n",
        "def get_passionate_redditors(df):\n",
        "  newdf = df.copy()\n",
        "  newdf['full_text'] = newdf.apply(lambda row: f\"{row.title}. {row.selftext}\", axis=1)\n",
        "  newdf['full_text_len'] = newdf.apply(lambda row: len(row.full_text), axis=1)\n",
        "  newdf = newdf[['author', 'full_text', 'full_text_len']]\n",
        "  newdf = newdf.loc[newdf['full_text_len'] >= 1000]\n",
        "  newdf['ratio'] = newdf.apply(lambda row: ratio_cal(row), axis=1)\n",
        "  newdf.drop_duplicates(subset=['author'],inplace=True)\n",
        "  sorted_df = newdf.sort_values(by=['ratio'], ascending=False)[['author', 'ratio']].head(100)\n",
        "  return_list = sorted_df.apply(lambda row: (row.author, row.ratio), axis=1).tolist()\n",
        "\n",
        "  return return_list"
      ],
      "metadata": {
        "id": "yjfpDjS2oPzP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_passionate_redditors(df)"
      ],
      "metadata": {
        "id": "7ddl-35Trg2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91140cef-576a-400a-c8a8-a64fd76f8b13"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('healrstreettalk', 0.030053034767236298),\n",
              " ('CommonEmployment2', 0.02894593118514473),\n",
              " ('nyello-2000', 0.027966742252456538),\n",
              " ('Kinmuan', 0.0273190621814475),\n",
              " ('lilshawnyy420', 0.02670971529204579),\n",
              " ('I_am_a_[***]_to_ants', 0.02555066079295154),\n",
              " ('Tripmooney', 0.024944974321349962),\n",
              " ('Shovel_Wielder', 0.024211968935587025),\n",
              " ('seamslegit', 0.02363112391930836),\n",
              " ('th3allyK4t', 0.023616236162361623),\n",
              " ('GeAlltidUpp', 0.023065476190476192),\n",
              " ('secretymology', 0.022627737226277374),\n",
              " ('Keppelin', 0.022242817423540315),\n",
              " ('Ninten-Doh', 0.022103658536585365),\n",
              " ('snorken123', 0.02151394422310757),\n",
              " ('Playaguy', 0.02112676056338028),\n",
              " ('OmniusQubus', 0.020893531895976883),\n",
              " ('DogMeatTalk', 0.020737327188940093),\n",
              " ('SSBM_Cob', 0.02072538860103627),\n",
              " ('blue4029', 0.02053915275994865),\n",
              " ('Stoaticor', 0.020492418555772406),\n",
              " ('ITehJelleh', 0.020477815699658702),\n",
              " ('FringeCenterPodcast', 0.02024446142093201),\n",
              " ('kanye5150', 0.02022058823529412),\n",
              " ('BornOnADifCloud', 0.020117351215423303),\n",
              " ('rrixham', 0.02002053388090349),\n",
              " ('kay278', 0.01998262380538662),\n",
              " ('EMB1981', 0.019880034275921166),\n",
              " ('sorryimveryhigh', 0.019776440240756664),\n",
              " ('AnakinWayneII', 0.019762845849802372),\n",
              " ('Travis-Cole', 0.019752199676782187),\n",
              " ('Coleblade', 0.019662921348314606),\n",
              " ('leeyuiwah', 0.01957713390759593),\n",
              " ('D12TRG', 0.019337016574585635),\n",
              " ('SecretAgentIceBat', 0.019055720969800086),\n",
              " ('DaRandomGitty2', 0.01904761904761905),\n",
              " ('NewTsahi', 0.019009216589861752),\n",
              " ('sbpotdbot', 0.018929503916449087),\n",
              " ('___TheKid___', 0.018850987432675045),\n",
              " ('enterprisevalue', 0.018796992481203006),\n",
              " ('lanqian', 0.01853932584269663),\n",
              " ('clemaneuverers', 0.018304431599229287),\n",
              " ('BrunoofBrazil', 0.018234165067178502),\n",
              " ('society0', 0.0180623973727422),\n",
              " ('RaoulDuke209', 0.01795472287275566),\n",
              " ('35quai', 0.017895185342991053),\n",
              " ('120inn[***]', 0.017849898580121704),\n",
              " ('nfk42', 0.017641597028783658),\n",
              " ('Venus230', 0.01763508930590097),\n",
              " ('s4nskrit', 0.017584451642757983),\n",
              " ('backpackwayne', 0.01754811580139091),\n",
              " ('kent_k', 0.01738334858188472),\n",
              " ('Long_on_AMD', 0.01725625539257981),\n",
              " ('marvi444', 0.017180094786729858),\n",
              " ('The_In-Betweener', 0.01686634872629987),\n",
              " ('bgny', 0.016853932584269662),\n",
              " ('greyuniwave', 0.01672998733586616),\n",
              " ('yellowsnow2', 0.0167186171720034),\n",
              " ('notinferno', 0.016659262549977787),\n",
              " ('KarmaFury', 0.0166270783847981),\n",
              " ('BK-Vatras', 0.01651615969581749),\n",
              " ('BUDDHAPHISH', 0.0165016501650165),\n",
              " ('Pretty_iin_Pink', 0.01642608262817322),\n",
              " ('ThereGoesJoe', 0.01639344262295082),\n",
              " ('CertainTour', 0.01634877384196185),\n",
              " ('CharlieXBravo', 0.01631116687578419),\n",
              " ('Scyllarious', 0.016296500034380802),\n",
              " ('allofusahab', 0.016260162601626018),\n",
              " ('venCiere', 0.016215147320545777),\n",
              " ('Cross_Ange', 0.016196104180345807),\n",
              " ('justarandomguy07', 0.016144349477682812),\n",
              " ('ThePoarter', 0.01613770844540075),\n",
              " ('CuteBananaMuffin', 0.016120094705556396),\n",
              " ('MaxDemonNoir', 0.01607050285121825),\n",
              " ('_NoSoup4You', 0.016057585825027684),\n",
              " ('theinfinitelight', 0.016045845272206302),\n",
              " ('edgar-reed', 0.016037735849056604),\n",
              " ('north0east', 0.015942028985507246),\n",
              " ('FeedTheeTrees', 0.01591695501730104),\n",
              " ('Cloud9Shopper', 0.01583493282149712),\n",
              " ('Zendexor', 0.01582815149802148),\n",
              " ('xrangegod1', 0.015792798483891344),\n",
              " ('factfind', 0.01572707476409388),\n",
              " ('FloundersEdition', 0.015710545108307546),\n",
              " ('truthesda', 0.01566871852266368),\n",
              " ('retalaznstyle', 0.015651201788708775),\n",
              " ('scamaltert', 0.01560110125420618),\n",
              " ('bionista', 0.015561569688768605),\n",
              " ('mendelevium34', 0.015503875968992248),\n",
              " ('41f4', 0.01527331189710611),\n",
              " ('LowKiwi4', 0.015202702702702704),\n",
              " ('Kwindecent_exposure', 0.015071590052750565),\n",
              " ('reddit_loves_pedos', 0.015070154165944916),\n",
              " ('joyousjoyness', 0.015056818181818182),\n",
              " ('CaspianX2', 0.014979029358897543),\n",
              " ('Stevemagegod', 0.014950166112956811),\n",
              " ('Konradleijon', 0.014908256880733946),\n",
              " ('dontbuyanylogos', 0.01490467937608319),\n",
              " ('lukeu42', 0.014844804318488529),\n",
              " ('BlindingTwilight', 0.01466544454628781)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## P1.3 Ethics (10 marks)"
      ],
      "metadata": {
        "id": "jQKJ4zc9UyOc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsAF9jpblJLp"
      },
      "source": [
        "Imagine you are **the head of a data mining company** that needs to use the insights gained in this assignment to scan social media for covid-related content, and automatically flag it as conspiracy or not conspiracy (for example, for hiding potentially harmful tweets or Facebook posts). Some\n",
        "information about the project and the team:\n",
        "\n",
        "- Your client is a political party concerned about misinformation.\n",
        "- The project requires mining Facebook, Reddit and Instagram data.\n",
        "- The team consists of Joe, an American mathematician who just finished college; Fei, a senior software engineer from China; and Francisco, a data scientist from Spain.\n",
        "\n",
        "Reflect on the impact of exploiting data science for such an application. You should map your discussion to one of the five actions outlined in the UKâ€™s Data Ethics Framework.\n",
        "\n",
        "Your answer should address the following:\n",
        "- Identify the action in which your project is the weakest.\n",
        "- Then, justify your choice by critically analyzing the three key principles for that action outlined\n",
        "in the Framework, namely transparency, accountability and fairness.\n",
        "- Finally, you should propose one solution that explicitly addresses one point related to one of these three principles, reflecting on how your solution would improve the data cycle in this particular use case.\n",
        "\n",
        "Your answer should be between 500 and 700 words. **You are strongly encouraged to follow a scholarly approach, e.g., with references to peer reviewed publications. References do not count towards the word limit**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YJQSO8Amuea"
      },
      "source": [
        "---\n",
        "\n",
        "Your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are the head of a Data Mining Company and we are given the assignment to scan social media for covid-related content, and automatically flag it as conspiracy or not conspiracy (for example, for hiding potentially harmful tweets or Facebook posts). We look at the five actions outlined in the UK Data Ethics Framework and address the issue.\n",
        "Facebook, Reddit, Instagram are one of the 3 most popular social media platforms in the United Kingdom where people consume most of their data.\n",
        "\n",
        "Covid-19 was one of the deadliest virusus which affected the entire world and even affects peope to this day.During this period a lot of false information,lies and conspiracies were posted on news networks as well as social networks.There were hardly few trusted sources but the content was most of the time misinformation.\n",
        "Any political party in power would be concerned if the people recieved false information as it may lead to them being removed out of power and criticised for spreading misinformation.\n",
        "\n",
        "The Data Ethics Framework guides appropriate and responsible data use in government and the wider public sector.The framework is split into overarching\n",
        "principles and specifc actions.Overarching principles are applicable throughout the entire process and underpin all actions and all aspects of the project. In addition, the framework provides specifc actions you can take at\n",
        "each stage of the project to advance transparency, accountability, and fairness. \n",
        "The 5 specific actions include:\n",
        "1. Define and understand public benefit and user need .\n",
        "2. Involve Diverse Expertise.\n",
        "3. Comply with the law.\n",
        "4. Review the quality and limitations of the Data.\n",
        "5. Evaluate and consider wider policy implication.\n",
        "\n",
        "\n",
        "**Review the quality and limitations of the Data:**\n",
        "From my understanding the project is weakest when it comes to this action.\n",
        "Insights from new technology are only as good as the data and practices used to create them. We must ensure that the data for the project is accurate, representative, proportionally used, of good quality, and that you are able to\n",
        "explain its limitations.\n",
        "Information on the origin, the type of virus, its effects and the number of people being affected were falsely interepreted on social media and people were unaware of the dangers of the virus until it became a pandemic. The quantity of content was more than the quality and people social media played a key factor in miscontruing information.\n",
        "Key things for us to understand:\n",
        "What data source(s) is being used?\n",
        "â— Are individuals and/or organisations providing the data aware of how it will be used? If the user is repurposing data for analysis without individual consent, how have you ensured that the new purpose is compatible with the original reason for collection (Article 6 (4) GDPR)?\n",
        "â— Are all metadata and field names clearly understood?\n",
        "â— Do we understand how the data for the project is generated? Remember that\n",
        "depending on where the data came from, the field may not represent what the field name/metadata indicates.\n",
        "â— What processes do you have in place to ensure and maintain data integrity?\n",
        "â— What are the caveats? How will the caveats be taken into account for any future policy or service which uses this work as an evidence base?\n",
        "â— Would using synthetic data be appropriate for the project? Synthetic data is entirely fabricated or abstracted from real data through various processes, e.g. anonymisation or record switching. It is often created with specifc features to test or train an algorithm.(Source-Data Ethics Framework UK government)\n",
        "Our Project is getting biased information which can cause confusion and result in spread of misinformation.\n",
        "\n",
        "Therefore we need to look into the Transparency, Accountability and the Fairness of the issue.\n",
        "\n",
        "Transparency means that your actions, processes and data are made open to inspection by publishing information about the project in a complete, open,\n",
        "understandable, easily-accessible, and free format.\n",
        "\n",
        "Accountability means that there are effective governance and oversight mechanisms for any project. Public accountability means that the public or\n",
        "its representatives are able to exercise effective oversight and control over\n",
        "the decisions and actions taken by the government and its officials, in\n",
        "order to guarantee that government initiatives meet their stated objectives\n",
        "and respond to the needs of the communities they are designed to benefit. \n",
        "\n",
        "Fairness â€” It is crucial to eliminate your projects potential to have unintended discriminatory effects on individuals and social groups. You should aim to mitigate biases which may infuence your models outcome and ensure that the project and its outcomes respect\n",
        "the dignity of individuals, are just, nondiscriminatory, and consistent with the public interest, including human rights and democratic values. (Source: Understanding artifcial intelligence ethics and safety guide developed by the Government Digital Service and the Offce for Artifcial Intelligence.) \n",
        "\n",
        "**Transparency**\n",
        "When it comes to the quality of data, we need to make sure that we do not spread biased information and reveal only the truth. Sometimes teh information might affect certain individuals, community or might even tarnish certain political parties or the reputation of governments that might sway the way how people look at them, for this reason the information could be changed in order to favor others. This should not be done. \n",
        "Information should be freely available to people. Incase of covid-19 many governments tried to reduce the information by playing the pandemic down and saying that covid was a hoax and it was a blatant lie and this led to many people losing their lives after believing the government.\n",
        "Thus hiding the conspiracies from unverified sources and revealing correct information with no bias is crucial for Transparency and for making sure that information is fair, just and honest. The three engineers can come together in scrapping data on coivd, then checking if it a trusted and the stats are accurate and make sure that the data with their correct and transparent information is available easily for the people.\n",
        "The automatic classification of content as conspiracy or not could be harmful if false information is classified as non conspiracy and spreads misinformation. The team must be open and honest about how they are developing the algorithm and what criteria they use to flag content.\n",
        "Explain what your project does and how it was designed in plain language to a non-expert audience.\n",
        "Describe the process and the aim of your algorithm, as well as what variables are used for what outcomes without\n",
        "using technical terms.\n",
        "Make this explanation publicly available (e.g. on GitHub, blogs, or gov.uk)\n",
        "\n",
        "**Accountability**\n",
        "The government and various organizations need to be accountable when it comes to information being spread. If at all the information is a conspiracy theory, they should take the initative to apologize and correct their mistakes.Long-term oversight and public scrutiny mechanisms\n",
        "are built into the project cycle. Our project needs to be held accountable that every information is honest and has enough trusted sources. The engineers can be accountable to each other to check if every issue raised is a conspiracy and check various different sources. \n",
        "Anonymous data should be thoroughly checked with proof for the political party to be accountable if spreading the information.\n",
        "There must be systems in place to hold the team accountable if false information is identified as not a conspiracy and allowed to propagate. This can involve periodic reviews of the algorithm's effectiveness or channels for people to contest the flagging of their content.\n",
        "When sharing models its important that it does not endanger either the:\n",
        "â— privacy of those whose data was used to train it\n",
        "â— integrity of the task being undertaken. \n",
        "\n",
        "Sometimes the data might have missing values and thus the source from which the data is exported must be properly cleaned before displaying the end results.\n",
        "\n",
        "**Fairness**\n",
        "The data should be fair and just. There should be no bias on a particular group of people/organization/country/government. \n",
        "The project promotes just and equitable outcomes,has negligible detrimental effects and is aligned with human rights conditions.\n",
        "Each and every organization might release information which is more aligned to their countries people as the majority of people using social media in a particular country will get more regional biased content.The automated classification of content as conspiratorial or not could unfairly disadvantage some people or groups. The team must take into account any biases that might exist in their algorithm and make sure that no content is being flagged incorrectly. \n",
        "This leads to difference in opinions and unfairness between different groups and organizations.\n",
        "\n",
        "**SOLUTION**\n",
        "For Transparency, we use Explainability which is the extent to which the workings in a machine learning algorithm can be explained in human terms. It means expanding on the transparency of what variables are used to provide information on how the algorithm came to give an output, and how changing the inputs can change the output. \n",
        "The team must be open and honest about how they are developing the algorithm and what criteria they use to flag content. \n",
        "\n",
        "For Accountability, \n",
        "Long-term oversight and public scrutiny mechanisms are built into the project cycle. Our project needs to be held accountable that every information is honest and has enough trusted sources. The engineers can be accountable to each other to check if every issue raised is a conspiracy and check various different sources. The 3 engineers can check and be accountable to each other making sure everything is accurate and nothing is false.\n",
        "\n",
        "For Fairness,\n",
        "The automated classification of content as conspiratorial or not could unfairly disadvantage some people or groups. The team must take into account any biases that might exist in their algorithm and make sure that no content is being flagged incorrectly.  They should check other government information and other trusted sources before the information is released.\n",
        "(Source:Fighting COVID-19 Misinformation on Social Media: Experimental Evidence for a Scalable Accuracy-Nudge Intervention Gordon Pennycook)\n",
        "\n",
        "\n",
        "**Conclusion**\n",
        "Incorporating human review into the automated flagging process is one way to optimise the data cycle in this particular use case. To make sure that judgements are being made correctly, this can entail having a group of moderators examine content that has been flagged. By giving people a clear awareness of how their data is being used, ensuring that the team is held accountable for their actions, and eliminating unfair targeting or discrimination, this solution addresses the values of openness, accountability, and fairness.\n"
      ],
      "metadata": {
        "id": "wPBNWsgRtQfL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nzvnLVFPMPtA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}