{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CmjxlpGkItr"
      },
      "source": [
        "# Practical Session 2\n",
        "This session consists of some first steps of a machine learning process:\n",
        "\n",
        "*   loading data\n",
        "*   data exploration\n",
        "*   data pre-processing including\n",
        "    * dealing with missing values\n",
        "    * encoding categorical features\n",
        "    * feature scaling\n",
        "\n",
        "In this practical, we will use the well used Titanic data.\n",
        "\n",
        "Python packages used in this practics:\n",
        "* sklean\n",
        "* pandas\n",
        "* matplotlib\n",
        "* seaborn\n",
        "\n",
        "Author: Yuhua Li\n",
        "\n",
        "Date:   November 2022 updated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJ-6oXKtkBok"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTsOIpKnb2cB"
      },
      "source": [
        "# Get data\n",
        "\n",
        "Get the Titanic data and have a simple inspection of the data\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlAAVesExbA0"
      },
      "outputs": [],
      "source": [
        "# load Titanic data from URL\n",
        "titanic = pd.read_csv('http://bit.ly/kaggletrain')\n",
        "#titanic = pd.read_csv('https://raw.githubusercontent.com/justmarkham/pandas-videos/master/data/titanic_train.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE1cR08HckyQ"
      },
      "source": [
        "Have an initia inspection of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73Eo49s0xgyX"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None, 'max_colwidth', None, 'display.expand_frame_repr', False) # print all columns in full, prevent line break\n",
        "\n",
        "print('\\n--- Show the number of data points (rows) and features (columns)---\\n', titanic.shape)\n",
        "\n",
        "print('\\n---Information of the titanic dataset --- \\n')\n",
        "print(titanic.info())\n",
        "print('\\n ---Column names of the dataset --- \\n', titanic.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Look at the first 10 and the last 5 lines of the dataset\n",
        "print('\\nBelow is the first 10 lines of the dataset......\\n', titanic.head(10))\n",
        "print('\\n\\nBelow is the last 5 lines of the dataset......\\n', titanic.tail(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the statistics of the dataset. Note it only show the results for features with neumerical values\n",
        "print('\\nBelow is the statistics of the dataset......\\n\\n', titanic.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LrB3DENzT3O"
      },
      "source": [
        "### Remove features that are not relevant to modelling\n",
        "\n",
        "The dataset contains columns for passenger IDs, names and ticket numbers, which aren't useful for machine learning modelling. So we can remove 'PassengerId', 'Name', 'Ticket' columns from the original dataset. Note  generally we need to remove personally identifiable information from a dataset to avoid violation of the law of GDPR (Genrla Data Protection Regulation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tatxNI_R0eR3"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  titanic.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace = True)\n",
        "except KeyError:\n",
        "  print('Attributes already removed')\n",
        "\n",
        "print(titanic.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5kJgvoSfwz7"
      },
      "source": [
        "# Dealing with missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iUFXRg1gFtR"
      },
      "source": [
        "We first check if there are missing values in each feature, 0 for no missing value or a number for the number of missing values. You will see there missing values for Age, Cabin and Embarked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOcc_sjmtwbY"
      },
      "outputs": [],
      "source": [
        "titanic.isna().sum()   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQu6EUA0hc1R"
      },
      "source": [
        "As you can see from above, there are missing values in this Titanic dataset. \n",
        "\n",
        "Some packages of machine learning methods provide the ability of dealing with a dataset with missing values so they can take the data directly without us explicitly dealing with missing values. However, many others need us to process missing values before feeding the dataset for machine leanring modelling.\n",
        "\n",
        "You have a few options to deal with missing values:\n",
        "1. Get rid of the corresponding instances.\n",
        "2. Get rid of the whole attribute.\n",
        "3. Set the values to some value (zero, the mean, the median, etc.).\n",
        "4. Use imputation methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjd9PhFdn9Zx"
      },
      "source": [
        "## 1. Get rid of the instances (data points) that contain missing values. \n",
        "If the number of instances in a dataset is large and the fraction of instances with missing values is small, an easy way is simply to remove those instances containing missing values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYluqRpO4RZW"
      },
      "outputs": [],
      "source": [
        "print('Data size BEFORE deleteting instances with missing values: ', titanic.shape)\n",
        "\n",
        "titanic_ins = titanic.dropna(subset=['Age', 'Cabin', 'Embarked'])   # delete all instances that have missing values for the features of Age, Cabin, and Embarked\n",
        "print('\\nData size AFTER deleteting instances containing missing values: ', titanic_ins.shape)\n",
        "titanic_ins.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g713KJ878VCo"
      },
      "source": [
        "## 2. Get rid of the whole attribute.\n",
        "\n",
        "As seen above, the feature Cabin contains 687 missing values which is a significant portion of the total instances (891). If we remove those instances based on the Cabin feature (and Age and Embarked), we only get 183 instances left from the original 891 instances, which means we lost the majority portion of the original dataset. So we'd better drop the feature of *Cabin* entirely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUsR0ZhzAh__"
      },
      "outputs": [],
      "source": [
        "titanic1 = titanic.drop(\"Cabin\", axis=1)\n",
        "print(titanic1.head())\n",
        "titanic1.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taweZjZhEzbD"
      },
      "source": [
        "## 3. Set the missing values to some value\n",
        "After removing the entire feature of *Cabin*, the resulting dataset ***titanic1*** still contains missing values for *Age* and *Embarked*. We may fill the missing values by statistics, e.g., mean and median of a numeric feature or the most frequent value of a categorical feature, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3rYmdwtGvT2"
      },
      "outputs": [],
      "source": [
        "# Age is a numeric feature, we may replace missign values by the median of Age \n",
        "median = titanic1[\"Age\"].median() # option 3\n",
        "titanic1[\"Age\"].fillna(median, inplace=True)\n",
        "print('After filling missing values of Age\\n', titanic1.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVB8AQc5XHv2"
      },
      "source": [
        "## 4. Use imputation methods.\n",
        "sci-kit learn provide many imputation methods. Here we replace missing values of *Embarked* using its mode, i.e., its most frequent value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2a5CfDTXzdd"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer    # For more information about SimpleImputer, see https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "imputer.fit(titanic1)\n",
        "embarked = imputer.transform(titanic1)\n",
        "\n",
        "#print(embarked[:20, ])\n",
        "print(embarked.shape)\n",
        "print(pd.DataFrame(embarked).isna().sum())\n",
        "pd.DataFrame(embarked).info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQkdN_-8i_db"
      },
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGaBY4GXjFKr"
      },
      "source": [
        "## Scatter plot\n",
        "\n",
        "Scatter plot shows data distribution of a pair of features, it can visuallly reveal the relationship between a feature pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHSPYTLnYxYx"
      },
      "outputs": [],
      "source": [
        "# use scatter_matrix of pandas.plotting     https://pandas.pydata.org/docs/reference/plotting.html\n",
        "from pandas.plotting import scatter_matrix\n",
        "scatter_matrix(titanic1[['Age', 'SibSp', 'Parch', 'Fare']], figsize=(12, 8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61YdvhXskF9U"
      },
      "outputs": [],
      "source": [
        "# We can also use plot of pandas DataFrame to plot a pair of features: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html\n",
        "titanic1.plot(kind='scatter', x='Age', y='Fare')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_JUUgqF-X-g"
      },
      "source": [
        "## Box plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6193lUo-cB9"
      },
      "outputs": [],
      "source": [
        "# We demonstrate boxplot for 'Age', 'Fare'. boxplot can visualise the distribution of a feature for outlies removal if needed.\n",
        "\n",
        "print(titanic1[['Age', 'Fare']].describe())\n",
        "titanic1[['Age', 'Fare']].boxplot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fDUyBxFlh1p"
      },
      "source": [
        "## Correlation\n",
        "The correlation coefficient measures the linear relationship between a pair of variables, range [-1, 1]. 1 indicates a full linear relationship, 0 no linear relationship, and -1 full negatively linear relationship. It can be used for feature selection to remove redundant features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHBrlYX7lok1"
      },
      "outputs": [],
      "source": [
        "corr_matrix = titanic1.corr()\n",
        "print(corr_matrix)\n",
        "\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(corr_matrix, vmax=0.9, square=True, annot=True, linewidths=0.3, cmap=\"YlGnBu\", fmt=\".1f\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phhIiHvmmAlJ"
      },
      "source": [
        "## Histogram\n",
        "We use the histogram to see what a feature distribution looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30YAnXC5mE-4"
      },
      "outputs": [],
      "source": [
        "titanic1['Age'].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqu0cCEBr4MX"
      },
      "source": [
        "# Feature scaling\n",
        "Feature scaling is a necessary step for most machine learning methods in order to achieve good learning performance and a faster learning process.\n",
        "\n",
        "There are two commonly used feature scaling methods: \n",
        "1. Scaling features to a defined range: e.g., min-max scaling\n",
        "2. Standardization: zero mean and unit variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARP3fjyYyjGh"
      },
      "source": [
        "## 1. Scaling features to a range that you define\n",
        "Commonly used ranges are [0, 1] and [-1, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJJtkd1qAf-5"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "minMax_scale = MinMaxScaler()   # to default range [0, 1]\n",
        "titanic1['Age'] = minMax_scale.fit_transform(titanic1[['Age']])\n",
        "\n",
        "titanic1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCK6BDMpD9k1"
      },
      "source": [
        "## Standardization (z-score)\n",
        "Standardization scales a featue to a feature with 0 mean and 1 standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3ptzvvEDTZq"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "standard_scale = StandardScaler()   # to default 0 mean and 1 standard deviation\n",
        "titanic1['Fare'] = standard_scale.fit_transform(titanic1[['Fare']])\n",
        "\n",
        "titanic1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO9pSJCU8HzF"
      },
      "source": [
        "# Encoding categorical features\n",
        "\n",
        "Most machine learning methods take only numerical data, except decision tree based methods which can take numerical and categorical features directly. So we need to convert categories to numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgqdRLL48VuK"
      },
      "source": [
        "## One hot encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSwKtvOr8avC"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "titanic1 = titanic[['Pclass', 'Sex', 'Embarked']].dropna()\n",
        "#print(titanic1.info())\n",
        "print('\\n---The original data---\\n', titanic1.head())\n",
        "\n",
        "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "enc.fit(titanic1)\n",
        "\n",
        "print('\\n', enc.categories_)\n",
        "\n",
        "titanic2 = pd.DataFrame(enc.transform(titanic1))\n",
        "titanic2.columns = np.concatenate(enc.categories_).ravel().tolist()\n",
        "print('\\n---The data after one hot encoding---\\n', titanic2.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRCO8oBbDslK"
      },
      "source": [
        "## Effect encoding\n",
        "A feature with N categories will produce N binary features after one hot encoding as above. One of the N binary features is perfectly collinear with the other N-1 features. This means there are only N-1 non-collinear binary features for a N-category feature. So we can drop one of the derived binary features as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vkSzbuIDhPq"
      },
      "outputs": [],
      "source": [
        "enc = OneHotEncoder(drop='first', sparse=False)\n",
        "titanic1 = titanic[['Pclass', 'Sex', 'Embarked']].dropna()\n",
        "print(titanic1.head())\n",
        "enc.fit(titanic1)\n",
        "titanic2 = pd.DataFrame(enc.transform(titanic1))\n",
        "print(titanic2.shape)\n",
        "\n",
        "print('\\n---The data after one hot encoding---\\n', titanic2.head())\n",
        "\n",
        "titanic2 = pd.concat([titanic2, titanic[['Age', 'SibSp',  'Parch', 'Fare']]], axis=1)\n",
        "titanic2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8lzZlWUfYhq"
      },
      "source": [
        "## Use Transformer\n",
        "Or use make_column_transformer to encode the named categorical features and keep other features unchanged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsSL3IEugLKx"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "imputer.fit(pd.DataFrame(titanic1[\"Embarked\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "titanic3 = titanic[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "print('Before filling missing values of Age\\n', titanic3.isna().sum())\n",
        "print(titanic3.shape)\n",
        "print(titanic3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JcWENd6NagB"
      },
      "outputs": [],
      "source": [
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "titanic3[\"Embarked\"] = imputer.fit_transform(pd.DataFrame(titanic3[\"Embarked\"]))\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "titanic3[\"Age\"] = imputer.fit_transform(pd.DataFrame(titanic3[\"Age\"]))\n",
        "print('After filling missing values of Age\\n', titanic3.isna().sum())\n",
        "\n",
        "print(titanic3.shape)\n",
        "print(titanic3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "col_trans = make_column_transformer(\n",
        "    (SimpleImputer(strategy=\"most_frequent\", add_indicator=True), [\"Embarked\"]),\n",
        "    (SimpleImputer(strategy=\"median\", add_indicator=True), [\"Age\"]),\n",
        "    (OneHotEncoder(), ['Sex', 'Embarked']),\n",
        "    remainder = 'passthrough')\n",
        "\n",
        "titanic_ce = col_trans.fit_transform(titanic3)\n",
        "\n",
        "np.set_printoptions(threshold=np.inf, linewidth=np.inf, suppress=True, precision=2)\n",
        "print(titanic_ce[0:20, ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W50ActG5-2me"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "### Tips\n",
        "There are many packages at our disposal. It is nearly impossible for us to remember all functionalities and details of every package. A good practice is always to consult the package documentation first when you have questions about the use of a package and its methods or algorithms.\n",
        "\n",
        "So, have a look at sklearn.preprocessing for its main modules that you will use to preprocess data https://scikit-learn.org/stable/modules/preprocessing.html#\n",
        "\n",
        "Read the Category Encoders for other encoding methods at https://contrib.scikit-learn.org/category_encoders/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW2m5iwMevSa"
      },
      "source": [
        "# First machine learning project\n",
        "Finally we put things together to practice our first machine learning project, here we build a k-nerest neighbours model to predict Suvival based on passengers' data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tkmGLJBfHxL"
      },
      "source": [
        "## Get data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISWF_tuMHWEK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "\n",
        "# Get a new copy of titanic data. \n",
        "# We ignore 'PassengerId', 'Name' and 'Ticket', as they are useless for machine learnign.\n",
        "# We also ignore 'Cabin' as it contains too many missing values.\n",
        "titanic = pd.read_csv('http://bit.ly/kaggletrain')\n",
        "titanic.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace = True)\n",
        "\n",
        "pd.set_option('display.max_columns', None, 'max_colwidth', None, 'display.expand_frame_repr', False)\n",
        "print(titanic)\n",
        "\n",
        "# split the data into input and output\n",
        "titanic_x = titanic[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "titanic_y = titanic[['Survived']]\n",
        "\n",
        "print(titanic_x.head(20))\n",
        "#print(titanic_x['Embarked'].isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BuO71WdfTlG"
      },
      "source": [
        "## Split data into training set and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEQcalcmfbnn"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 70% of the data for traingin, 30% for test, i.e., test_size=0.3\n",
        "X_train, X_test, y_train, y_test = train_test_split(titanic_x, titanic_y, test_size=0.3, random_state=42, stratify=titanic_y) \n",
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8luxwUYG_o7"
      },
      "source": [
        "## Preprocessing Pipeline & ColumnTransformer\n",
        "We now put all transformations using Pipeline & ColumnTransformer. Doing so not only makes code tidy but also has advantages:\n",
        "\n",
        "*   allow to include the preprocessing steps in the hyperparameter tuning (will learn later)\n",
        "*   avoid data leakage, i.e., avoid making the mistake of using any test data for model training\n",
        "*   guarantee that your data is always preprocessed the same way. For example,  if a categorical feature has a category in the test set that does not occur in the training set or a category in the training set that doesn't occur in the test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl0M9DOcJVId"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "#from sklearn.compose import make_column_transformer\n",
        "\n",
        "# transformer for categorical features\n",
        "categorical_features = ['Pclass', 'Sex', 'Embarked']\n",
        "categorical_transformer = Pipeline(\n",
        "    [\n",
        "        ('imputer_cat', SimpleImputer(strategy = 'most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# transformer for numerical features\n",
        "numeric_features = ['Age', 'SibSp', 'Parch', 'Fare']\n",
        "numeric_transformer = Pipeline(\n",
        "    [\n",
        "        ('imputer_num', SimpleImputer(strategy = 'median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]\n",
        ")\n",
        "\n",
        "# combine them in a single ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    [\n",
        "        ('categoricals', categorical_transformer, categorical_features),\n",
        "        ('numericals', numeric_transformer, numeric_features)\n",
        "    ],\n",
        "    remainder = 'drop'\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "X_train_processed = preprocessor.fit_transform(X_train) # fit and transform X_train\n",
        "X_test_processed = preprocessor.transform(X_test) # transform X_test using the model fitted on X_train\n",
        "\n",
        "np.set_printoptions(threshold=np.inf, linewidth=np.inf, suppress=True, precision=2)\n",
        "\n",
        "print(X_train_processed[0:20, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybRi-22yp3sv"
      },
      "source": [
        "## Define the model\n",
        "We use a pipeline to put together the preprocessor from above and the k-nearest neighbours classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZqn1-AoqSyG"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "myClassfier = Pipeline(\n",
        "    [\n",
        "     ('preprocessing', preprocessor),\n",
        "     ('classifier', KNeighborsClassifier())\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zth4PnClryoI"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SO4C-Unr2Pg"
      },
      "outputs": [],
      "source": [
        "\n",
        "myClassfier.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9dbx26Or7gk"
      },
      "source": [
        "## Evaluate the model\n",
        "We evalute the performance of the trained classifier on test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRJ4jFOCr_BU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = myClassfier.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzbr88wr1umw"
      },
      "source": [
        "# Exercise\n",
        "Download Credit Approval Data Set from UCI Machine Learning Repsoitory. Do:\n",
        "\n",
        "- practicing exploratory data analysis\n",
        "- dealing with missing values if any\n",
        "- encoding categorical features\n",
        "- scaling features\n",
        "- if you have time, implementing a classifier to predict if a credit card application is approved (+ of the last column) or reject (- of the last column)\n",
        "\n",
        "You can read more information about the data set from https://archive.ics.uci.edu/ml/datasets/Credit+Approval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYNurIkY17b0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Get Credit Approval Data Set\n",
        "crx = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data', header='infer')\n",
        "crx.columns = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'Target']\n",
        "\n",
        "# Start writing your IPython notebook............\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "CMT307_lab1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
